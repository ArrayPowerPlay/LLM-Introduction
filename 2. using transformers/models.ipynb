{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a435686c",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c62986",
   "metadata": {},
   "source": [
    "In this section, weâ€™ll take a closer look at creating and using models. Weâ€™ll use the `AutoModel` class, which is handy when you want to instantiate any model from a checkpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b1c56",
   "metadata": {},
   "source": [
    "## 1. Creating a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581cba0d",
   "metadata": {},
   "source": [
    "Letâ€™s begin by examining what happens when we instantiate an `AutoModel`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e7f363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704ec365e5d34908bb95e1a847ff7317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ff1d5a87764489b27e7887949a307b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9a31517db74597a21b633a4f5a0424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-cased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea987d",
   "metadata": {},
   "source": [
    "Similar to the tokenizer, the `from_pretrained()` method will download and cache the model data from the Hugging Face Hub. As mentioned previously, the checkpoint name corresponds to a specific model architecture and weights, in this case a BERT model with a basic architecture (12 layers, 768 hidden size, 12 attention heads) and cased inputs (meaning that the uppercase/lowercase distinction is important). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8dc673",
   "metadata": {},
   "source": [
    "The `AutoModel` class and its associates are actually simple wrappers designed to fetch the appropriate model architecture for a given checkpoint. Itâ€™s an â€œautoâ€ class meaning it will guess the appropriate model architecture for you and instantiate the correct model class. However, if you know the type of model you want to use, you can use the class that defines its architecture directly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90950980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb703b",
   "metadata": {},
   "source": [
    "## 2. Loading and saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390a932",
   "metadata": {},
   "source": [
    "Saving a model is as simple as saving a tokenizer. In fact, the models actually have the same `save_pretrained()` method, which saves the modelâ€™s weights and architecture configuration:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f8e591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769d76d2a62442dea78257594cc3b27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feecedc",
   "metadata": {},
   "source": [
    "Saving a model is as simple as saving a tokenizer. In fact, the models actually have the same `save_pretrained()` method, which saves the modelâ€™s weights and architecture configuration:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5070e35c",
   "metadata": {},
   "source": [
    "The `model.safetensors` file is known as the state dictionary; it contains all your modelâ€™s weights. The two files work together: the configuration file is needed to know about the model architecture, while the model weights are the parameters of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e78d7",
   "metadata": {},
   "source": [
    "To reuse a saved model, use the from_pretrained() method again:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a010d678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3ee0291c3544d0845972690f409627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b2e9a",
   "metadata": {},
   "source": [
    "A wonderful feature of the ðŸ¤— Transformers library is the ability to easily share models and tokenizers with the community. To do this, make sure you have an account on Hugging Face. If youâ€™re using a notebook, you can easily log in with this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612a62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a534538",
   "metadata": {},
   "source": [
    "Then you can push the model to the Hub with the `push_to_hub()` method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e4c178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fce288e97e48c0a4d7565a3796ed54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bad9a8675d341e4b9c1065bb2994a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2757cbcd650447e18f276f89151aaaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/arraypowerplay/my-awesome-model/commit/c9c3c382605f725696bfaa3b05b7788edac5475e', commit_message='Upload model', commit_description='', oid='c9c3c382605f725696bfaa3b05b7788edac5475e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/arraypowerplay/my-awesome-model', endpoint='https://huggingface.co', repo_type='model', repo_id='arraypowerplay/my-awesome-model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"my-awesome-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00748ff",
   "metadata": {},
   "source": [
    "This will upload the model files to the Hub, in a repository under your namespace named `my-awesome-model`. Then, anyone can load your model with the `from_pretrained()` method!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e6ab31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf54eea81e748a99e9fe980c51fb6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--arraypowerplay--my-awesome-model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb586a01192e448a9fcafb40e89718e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f92d7e304f64933bcf4649de50872f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"arraypowerplay/my-awesome-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446abf4",
   "metadata": {},
   "source": [
    "## 3. Encoding text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e80d67",
   "metadata": {},
   "source": [
    "Transformer models handle text by turning the inputs into numbers. Here we will look at exactly what happens when your text is processed by the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b1caca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87672473372b487fb65c1bca3a772bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae436345f8ee483ba952fe7018c3ed0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f1c1a6686f42649db754f4ee307783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184f629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 146, 112, 182, 5193, 1103, 3044, 1115, 1128, 1138, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"I'm testing the knowledge that you have.\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f21f3",
   "metadata": {},
   "source": [
    "We get a dictionary with the following fields:\n",
    "\n",
    "* **input_ids**: numerical representations of your tokens\n",
    "\n",
    "* **token_type_ids**: these tell the model which part of the input is sentence A and which \n",
    "is sentence B (discussed more in the next section)\n",
    "\n",
    "* **attention_mask**: this indicates which tokens should be attended to and which should not (discussed more in a bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995e11d",
   "metadata": {},
   "source": [
    "We can decode the input IDs to get back the original text:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e49e0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] I ' m testing the knowledge that you have. [SEP]\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef08668",
   "metadata": {},
   "source": [
    "Youâ€™ll notice that the tokenizer has added special tokens â€” `[CLS]` and `[SEP]` â€” required by the model. Not all models need special tokens; theyâ€™re utilized when a model was pretrained with them, in which case the tokenizer needs to add them as that model expects these tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d85fb0",
   "metadata": {},
   "source": [
    "You can encode multiple sentences at once, either by batching them together (weâ€™ll discuss this soon) or by passing a list:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0960031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1327, 1110, 1122, 1176, 136, 102], [101, 146, 1274, 112, 189, 1221, 1299, 102], [101, 1109, 4250, 1110, 21162, 1105, 146, 1567, 1122, 106, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer([\"What is it like?\", \"I don't know man\", \"The weather is sunny and I love it!\"])\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425f371",
   "metadata": {},
   "source": [
    "Note that when passing multiple sentences, the tokenizer returns a list for each sentence for each dictionary value. We can also ask the tokenizer to return tensors directly from PyTorch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca7b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1731, 1132, 1128,  136,  102,    0,    0,    0,    0],\n",
      "        [ 101,  146,  112,  182, 2503,  117, 6243, 1128,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer([\"How are you?\", \"I'm fine, thank you!\"],\n",
    "                          padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dab6ff",
   "metadata": {},
   "source": [
    "But thereâ€™s a problem: the two lists donâ€™t have the same length! Arrays and tensors need to be rectangular, so we canâ€™t simply convert these lists to a PyTorch tensor (or NumPy array). The tokenizer provides an option for that: padding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e159605a",
   "metadata": {},
   "source": [
    "### 3.1. Padding inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2164ebca",
   "metadata": {},
   "source": [
    "If we ask the tokenizer to pad the inputs, it will make all sentences the same length by adding a special padding token to the sentences that are shorter than the longest one:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e2022f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1731, 1132, 1128,  136,  102,    0,    0,    0,    0],\n",
      "        [ 101,  146,  112,  182, 2503,  117, 6243, 1128,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\n",
    "    [\"How are you?\", \"I'm fine, thank you!\"], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5b016",
   "metadata": {},
   "source": [
    "Now we have rectangular tensors! Note that the padding tokens have been encoded into input IDs with ID 0, and they have an attention mask value of 0 as well. This is because those padding tokens shouldnâ€™t be analyzed by the model: theyâ€™re not part of the actual sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aad55b",
   "metadata": {},
   "source": [
    "### 3.2. Truncating inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd9520",
   "metadata": {},
   "source": [
    "The tensors might get too big to be processed by the model. For instance, BERT was only pretrained with sequences up to 512 tokens, so it cannot process longer sequences. If you have sequences longer than the model can handle, youâ€™ll need to truncate them with the `truncation` parameter:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de6c2666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1188, 1110, 170, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1263, 5650, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\n",
    "    \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\",\n",
    "    truncation=True,\n",
    ")\n",
    "print(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b1bce3",
   "metadata": {},
   "source": [
    "By combining the padding and truncation arguments, you can make sure your tensors have the exact size you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08ba07f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1731, 1132, 1128,  102],\n",
      "        [ 101,  146,  112,  182,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\n",
    "    [\"How are you?\", \"I'm fine, thank you!\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=5,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be839d9",
   "metadata": {},
   "source": [
    "### 3.3. Adding special tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e98df",
   "metadata": {},
   "source": [
    "Special tokens (or at least the concept of them) is particularly important to BERT and derived models. These tokens are added to better represent the sentence boundaries, such as the beginning of a sentence (`[CLS]`) or separator between sentences (`[SEP]`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88540e20",
   "metadata": {},
   "source": [
    "These special tokens are automatically added by the tokenizer. Not all models need special tokens; they are primarily used when a model was pretrained with them, in which case the tokenizer will add them since the model expects them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fb1fb",
   "metadata": {},
   "source": [
    "### 3.4. Using the tensors as inputs to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6840b494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1500,  0.2100,  0.2770,  ..., -0.1971,  0.2418, -0.1118],\n",
       "         [ 0.0130, -0.5233,  0.6800,  ..., -0.0204, -0.2299, -0.2385],\n",
       "         [ 0.1876, -0.4915, -0.2659,  ...,  0.0801, -0.4456,  0.9273],\n",
       "         [ 0.6587, -0.1734, -0.0356,  ..., -0.4110, -0.3439,  0.2987],\n",
       "         [ 0.8629,  0.1814,  0.1749,  ...,  0.0693,  1.2299, -0.2707]],\n",
       "\n",
       "        [[ 0.5111,  0.4098,  0.2642,  ..., -0.0678,  0.4584, -0.3087],\n",
       "         [ 0.5361,  0.1227,  0.4718,  ...,  0.0496,  0.3040,  0.1918],\n",
       "         [ 0.2898,  0.4114, -0.0288,  ..., -0.0173,  0.6955, -0.1688],\n",
       "         [ 0.2900, -0.0814,  0.1336,  ..., -0.1883,  0.2689, -0.0132],\n",
       "         [ 1.3207,  0.3622, -0.0432,  ...,  0.1485,  1.1995, -0.9229]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7487,  0.3865,  0.9999,  ...,  1.0000, -0.7804,  0.9881],\n",
       "        [-0.6507,  0.4607,  0.9999,  ...,  1.0000, -0.8657,  0.9938]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**encoded_input)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
