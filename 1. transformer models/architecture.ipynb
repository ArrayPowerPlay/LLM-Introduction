{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7da471",
   "metadata": {},
   "source": [
    "# Transformer Architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac7031",
   "metadata": {},
   "source": [
    "In this section, we’re going to dive deeper into the three main architectural variants of Transformer models and understand when to use each one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fc882f",
   "metadata": {},
   "source": [
    "## 1. Encoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfe228",
   "metadata": {},
   "source": [
    "Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having “bi-directional” attention, and are often called auto-encoding models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc6cfa",
   "metadata": {},
   "source": [
    "The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9311b4",
   "metadata": {},
   "source": [
    "Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d840f49",
   "metadata": {},
   "source": [
    "## 2. Decoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098630e5",
   "metadata": {},
   "source": [
    "Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4e39c",
   "metadata": {},
   "source": [
    "The pretraining of decoder models usually revolves around predicting the next word in the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d323446",
   "metadata": {},
   "source": [
    "These models are best suited for tasks involving text generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a1371",
   "metadata": {},
   "source": [
    "### Modern Large Language Models (LLMs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ee2af",
   "metadata": {},
   "source": [
    "Most modern Large Language Models (LLMs) use the decoder-only architecture. These models have grown dramatically in size and capabilities over the past few years, with some of the largest models containing hundreds of billions of parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec826cb",
   "metadata": {},
   "source": [
    "Modern LLMs are typically trained in two phases:\n",
    "\n",
    "1. Pretraining: The model learns to predict the next token on vast amounts of text data\n",
    "\n",
    "2. Instruction tuning: The model is fine-tuned to follow instructions and generate helpful responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c81dee",
   "metadata": {},
   "source": [
    "## 3. Sequence-to-sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9cfa5",
   "metadata": {},
   "source": [
    "Encoder-decoder models (also called sequence-to-sequence models) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b045c1",
   "metadata": {},
   "source": [
    "The pretraining of these models can take different forms, but it often involves reconstructing a sentence for which the input has been somehow corrupted (for instance by masking random words). The pretraining of the T5 model consists of replacing random spans of text (that can contain several words) with a single mask special token, and the task is then to predict the text that this mask token replaces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d051e9",
   "metadata": {},
   "source": [
    "Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
