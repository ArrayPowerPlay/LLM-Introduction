{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8d71a9",
   "metadata": {},
   "source": [
    "# What if my dataset isnâ€™t on the Hub?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a8895",
   "metadata": {},
   "source": [
    "## 1. Loading a local dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7024cf",
   "metadata": {},
   "source": [
    "For this example weâ€™ll use the SQuAD-it dataset, which is a large-scale dataset for question answering in Italian.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12299825",
   "metadata": {},
   "source": [
    "The training and test splits are hosted on GitHub, so we can download them with Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b1dc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SQuAD_it-test.json.gz', <http.client.HTTPMessage at 0x2561d3e5310>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url_train = \"https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\"\n",
    "url_test = \"https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\"\n",
    "urllib.request.urlretrieve(url_train, \"SQuAD_it-train.json.gz\")\n",
    "urllib.request.urlretrieve(url_test, \"SQuAD_it-test.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce1608",
   "metadata": {},
   "source": [
    "This will download two compressed files called `SQuAD_it-train.json.gz` and `SQuAD_it-test.json.gz`, which we can decompress as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6706c446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Decompress training file\n",
    "with gzip.open(\"SQuAD_it-train.json.gz\", \"rb\") as f_in:\n",
    "    with open(\"SQuAD_it-train.json\", \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Decompress test file\n",
    "with gzip.open(\"SQuAD_it-test.json.gz\", \"rb\") as f_in:\n",
    "    with open(\"SQuAD_it-test.json\", \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939872b",
   "metadata": {},
   "source": [
    "We can see that the compressed files have been replaced with `SQuAD_it-train.json` and `SQuAD_it-test.json`, and that the data is stored in the JSON format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ed16a",
   "metadata": {},
   "source": [
    "To load a JSON file with the `load_dataset()` function, we just need to know if weâ€™re dealing with ordinary JSON (similar to a nested dictionary) or JSON Lines (line-separated JSON). Like many question answering datasets, SQuAD-it uses the nested format, with all the text stored in a `data` field. This means we can load the dataset by specifying the `field` argument as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78b955a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0748939ce1e45d0a589724d2bc5f48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daded57",
   "metadata": {},
   "source": [
    "By default, loading local files creates a `DatasetDict` object with a `train` split. We can see this by inspecting the `squad_it_dataset` object:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907f7454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f96b1c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'paragraphs'],\n",
       "    num_rows: 442\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff70d4e",
   "metadata": {},
   "source": [
    "Great, weâ€™ve loaded our first local dataset! But while this worked for the training set, what we really want is to include both the `train` and `test` splits in a single `DatasetDict` object so we can apply `Dataset.map()` functions across both splits at once. To do this, we can provide a dictionary to the `data_files` argument that maps each split name to a file associated with that split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2478ca4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462f62f23cd94489ae37baec2c5b0884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1641f10185472b8f3fcc4c443bd320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
    "squad_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66be564",
   "metadata": {},
   "source": [
    "This is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf287d",
   "metadata": {},
   "source": [
    "The loading scripts in ðŸ¤— Datasets actually support automatic decompression of the input files, so we could have skipped the use of `gzip` by pointing the `data_files` argument directly to the compressed files:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
    "squad_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25df09",
   "metadata": {},
   "source": [
    "This can be useful if you donâ€™t want to manually decompress many GZIP files. The automatic decompression also applies to other common formats like ZIP and TAR, so you just need to point `data_files` to the compressed files and youâ€™re good to go!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f736c56",
   "metadata": {},
   "source": [
    "## 2. Loading a remote dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec6c10",
   "metadata": {},
   "source": [
    " Instead of providing a path to local files, we point the `data_files` argument of `load_dataset()` to one or more URLs where the remote files are stored. For example, for the SQuAD-it dataset hosted on GitHub, we can just point `data_files` to the SQuAD_it-*.json.gz URLs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
