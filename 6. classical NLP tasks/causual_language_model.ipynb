{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f1dbcb",
   "metadata": {},
   "source": [
    "# Training a causal language model from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac51df6",
   "metadata": {},
   "source": [
    "Up until now, weâ€™ve mostly been using pretrained models and fine-tuning them for new use cases by reusing the weights from pretraining. This is commonly referred to as transfer learning, and itâ€™s a very successful strategy for applying Transformer models to most real-world use cases where labeled data is sparse. In this chapter, weâ€™ll take a different approach and train a completely new model from scratch. This is a good approach to take if you have a lot of data and it is very different from the pretraining data used for the available models. However, it also requires considerably more compute resources to pretrain a language model than just to fine-tune an existing one. Examples where it can make sense to train a new model include for datasets consisting of musical notes, molecular sequences such as DNA, or programming languages. The latter have recently gained traction thanks to tools such as TabNine and GitHubâ€™s Copilot, powered by OpenAIâ€™s Codex model, that can generate long sequences of code. This task of text generation is best addressed with auto-regressive or causal language models such as GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf78c1",
   "metadata": {},
   "source": [
    "In this section we will build a scaled-down version of a code generation model: weâ€™ll focus on one-line completions instead of full functions or classes, using a subset of Python code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d27bc8",
   "metadata": {},
   "source": [
    "Here, weâ€™ll apply our tokenizer to a corpus of Python code derived from GitHub repositories. We will then use the `Trainer` API and ðŸ¤— `Accelerate` to train the model. Letâ€™s get to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307db90",
   "metadata": {},
   "source": [
    "## 1. Gathering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99f4064",
   "metadata": {},
   "source": [
    "Python code is abundantly available from code repositories such as GitHub, which we can use to create a dataset by scraping for every Python repository. This was the approach taken in the Transformers textbook to pretrain a large GPT-2 model. Using a GitHub dump of about 180 GB containing roughly 20 million Python files called `codeparrot`, the authors built a dataset that they then shared on the Hugging Face Hub.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247d62b",
   "metadata": {},
   "source": [
    "However, training on the full corpus is time- and compute-consuming, and we only need the subset of the dataset concerned with the Python data science stack. So, letâ€™s start by filtering the `codeparrot` dataset for all files that include any of the libraries in this stack. Because of the datasetâ€™s size, we want to avoid downloading it; instead, weâ€™ll use the streaming feature to filter it on the fly. To help us filter the code samples using the libraries we mentioned earlier, weâ€™ll use the following function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b49ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8aabb",
   "metadata": {},
   "source": [
    "Letâ€™s test it on two examples:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3de36779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\"\n",
    "\n",
    "print(\n",
    "    keyword_in_string(example_1, filters), keyword_in_string(example_2, filters)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076033e",
   "metadata": {},
   "source": [
    "We can use this to create a function that will stream the dataset and filter the elements we want:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2dc2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filter_dataset = defaultdict(list)\n",
    "    step = 0\n",
    "\n",
    "    for sample in tqdm(iter(dataset)):\n",
    "        step += 1\n",
    "        if keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filter_dataset[k].append(v)\n",
    "        print(f\"{len(filter_dataset['content']) / step:.2f} of data after filtering\")\n",
    "    return Dataset.from_dict(filter_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4cc0c2",
   "metadata": {},
   "source": [
    "Then we can simply apply this function to the streaming dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"\n",
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "dataset = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
    "filtered_data = filter_streaming_dataset(dataset, filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80298f",
   "metadata": {},
   "source": [
    "This leaves us with about 3% of the original dataset, which is still quite sizable â€” the resulting dataset is 6 GB and consists of 600,000 Python scripts!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19841e",
   "metadata": {},
   "source": [
    "Filtering the full dataset can take 2-3h depending on your machine and bandwidth. If you donâ€™t want to go through this lengthy process yourself, we provide the filtered dataset on the Hub for you to download:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40fc72e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc5c3fe19034b69bdd178b70b01d724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "codeparrot-ds-train.jsonl:   0%|          | 0.00/8.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\datasets--huggingface-course--codeparrot-ds-train. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a078a13f30c84879a52bffe6e1e54060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180d942225a14a8e9f496d648ec89022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "codeparrot-ds-valid.jsonl:   0%|          | 0.00/46.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\datasets--huggingface-course--codeparrot-ds-valid. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43820d0936f4ef1b0ce792d37969715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85aa466c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "    num_rows: 606720\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf176dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 606720\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 3322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,\n",
    "        \"validation\": ds_valid\n",
    "    }\n",
    ")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9eb61",
   "metadata": {},
   "source": [
    "Letâ€™s look at an example from the dataset. Weâ€™ll just show the first 200 characters of each field:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcc34e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: kmike/scikit-learn\n",
      "PATH: sklearn/utils/__init__.py\n",
      "COPIES: 3\n",
      "SIZE: 10094\n",
      "CONTENT: \"\"\"\n",
      "The :mod:`sklearn.utils` module includes various utilites.\n",
      "\"\"\"\n",
      "\n",
      "from collections import Sequence\n",
      "\n",
      "import numpy as np\n",
      "from scipy.sparse import issparse\n",
      "import warnings\n",
      "\n",
      "from .murmurhash import murm\n",
      "LICENSE: bsd-3-clause\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76113732",
   "metadata": {},
   "source": [
    "We can see that the `content` field contains the code that we want our model to train on. Now that we have a dataset, we need to prepare the texts so theyâ€™re in a format suitable for pretraining.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1833cd",
   "metadata": {},
   "source": [
    "## 2. Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e2390",
   "metadata": {},
   "source": [
    "The first step will be to tokenize the data, so we can use it for training. Since our goal is to mainly autocomplete short function calls, we can keep the context size relatively small. This has the benefit that we can train the model much faster and it requires significantly less memory. If it is important for your application to have more context (for example, if you want the model to write unit tests based on a file with the function definition), make sure you increase that number, but also keep in mind that this comes with a greater GPU memory footprint. For now, letâ€™s fix the context size at 128 tokens, as opposed to the 1,024 or 2,048 used in GPT-2 or GPT-3, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d90867",
   "metadata": {},
   "source": [
    "Most documents contain many more than 128 tokens, so simply truncating the inputs to the maximum length would eliminate a large fraction of our dataset. Instead, weâ€™ll use the `return_overflowing_tokens` option to tokenize the whole input and split it into several chunks. Weâ€™ll also use the `return_length` option to return the length of each created chunk automatically. Often the last chunk will be smaller than the context size, and weâ€™ll get rid of these pieces to avoid padding issues; we donâ€™t really need them as we have plenty of data anyway.\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "88cae0c0",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129388e",
   "metadata": {},
   "source": [
    "Letâ€™s see exactly how this works by looking at the first two examples:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2c95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b3a21f9cdf40b796953c2c7e05ded1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--huggingface-course--code-search-net-tokenizer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8388c48cb34476a99499df8b480d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8a3e97aa3e429babe85db5e8420c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411a0b75d35e4cc9ba67ccc50e99630e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5465effa62da46179a200a981b7378a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[280, 173, 2096, 310, 2025, 749, 24661, 14, 1377, 64, 1340, 6376, 8258, 4705, 2646, 14, 173, 280, 173, 173, 973, 4962, 978, 7187, 173, 173, 2745, 1601, 442, 635, 173, 973, 4985, 14, 6322, 978, 29638, 173, 2745, 3758, 173, 173, 973, 1428, 77, 26853, 250, 1817, 978, 288, 26853, 250, 1817, 19, 63, 1551, 173, 973, 1428, 6436, 978, 308, 352, 63, 1345, 63, 783, 12, 951, 63, 7194, 12, 4724, 63, 5293, 12, 373, 1387, 63, 536, 63, 7544, 12, 960, 18, 68, 12, 35001, 18, 68, 63, 228, 63, 22229, 12, 373, 35001, 18, 68, 63, 228, 63, 9422, 12, 6792, 63, 804, 63, 854, 63, 1345, 12, 373, 951, 63, 2437, 63, 977, 9, 173, 973, 1428, 692, 63, 2077, 978, 3115, 63], [692, 63, 2077, 173, 173, 612, 536, 612, 233, 2558, 77, 26853, 250, 1817, 19, 63, 1551, 485, 333, 352, 63, 1345, 63, 783, 485, 333, 1207, 63, 7194, 485, 333, 4048, 63, 5293, 485, 1366, 333, 5224, 63, 536, 63, 7544, 485, 333, 783, 18, 68, 485, 333, 11198, 18, 68, 63, 228, 63, 22229, 485, 1366, 333, 11198, 18, 68, 63, 228, 63, 9422, 485, 333, 3092, 63, 804, 63, 854, 63, 1345, 485, 333, 1207, 63, 2437, 63, 977, 485, 1366, 333, 4180, 63, 692, 63, 2077, 778, 173, 173, 3, 3596, 2926, 542, 12925, 549, 7784, 173, 7150, 14, 21848, 439, 10353, 485, 12925, 9, 4391, 173, 692, 5580, 8, 1023, 274, 232, 290, 12392, 292, 2391, 231, 753, 385, 1149, 442, 5580, 14], [312, 16376, 231, 2671, 1184, 256, 753, 300, 2072, 15, 1941, 1149, 300, 14200, 350, 232, 7078, 231, 2671, 292, 256, 5896, 14, 312, 507, 1154, 1480, 1041, 664, 371, 8415, 292, 256, 19475, 952, 232, 350, 256, 5896, 14, 2321, 26, 292, 811, 551, 461, 256, 713, 448, 296, 1480, 12, 4124, 232, 253, 309, 2039, 311, 26347, 26, 312, 705, 497, 15673, 14, 1377, 978, 5580, 232, 705, 5580, 323, 294, 7844, 26, 382, 18526, 232, 659, 24661, 14, 1377, 14, 11948, 624, 815, 44206, 312, 705, 1111, 11948, 323, 232, 1329, 509, 1791, 63, 1619, 794, 989, 232, 290, 312, 294, 25923, 497, 2297, 1193, 5184, 14, 2580, 14, 2129, 15, 607, 219, 15, 6545, 12392, 12532, 12, 232, 294, 1314, 461, 3729, 3099, 14], [312, 509, 1289, 1863, 4223, 248, 12, 1480, 7967, 222, 290, 222, 1174, 222, 1195, 222, 1480, 26, 671, 880, 292, 371, 2621, 292, 256, 19475, 3039, 298, 290, 222, 272, 14, 2196, 233, 1480, 312, 509, 1289, 1096, 4223, 248, 12, 1300, 274, 222, 264, 735, 8, 886, 12, 630, 274, 241, 302, 272, 337, 22733, 63, 692, 8, 886, 9, 222, 425, 26, 241, 302, 272, 337, 22733, 63, 4072, 8, 886, 9, 312, 509, 409, 22733, 63, 692, 8, 248, 12, 1429, 274, 222, 1347, 233, 333, 2425, 446, 83, 300, 5580, 2, 446, 1429, 814, 324, 612, 222, 264, 272, 14, 2196, 26, 241, 1347, 793, 20357, 446, 83, 2, 446, 272, 14, 2196, 298, 294, 7469, 26, 649, 1037, 7125, 4096, 1289, 1168], [612, 296, 2026, 1251, 4238, 222, 2884, 233, 1429, 814, 1863, 612, 298, 509, 5150, 1614, 426, 12, 655, 527, 274, 241, 3758, 14, 3092, 8, 1324, 12, 4176, 29, 24112, 9, 241, 302, 2884, 1614, 426, 12, 655, 527, 9, 222, 1429, 814, 1863, 612, 233, 5150, 298, 5150, 814, 324, 612, 233, 4488, 1863, 14055, 222, 5150, 814, 1276, 612, 233, 272, 337, 716, 63, 1276, 8, 1863, 814, 1276, 3485, 222, 5150, 14, 11948, 63, 4884, 233, 2884, 298, 302, 1429, 312, 509, 409, 22733, 63, 4072, 8, 248, 12, 5715, 274, 222, 290, 33531, 753, 5715, 280, 298, 1347, 233, 333, 4680, 446, 83, 300, 5580, 2, 446, 5715, 814, 324, 612, 222, 264, 272, 14, 2196, 26, 241, 1347, 793, 20357, 446, 83], [2, 446, 272, 14, 2196, 298, 509, 5150, 1614, 426, 12, 655, 527, 274, 241, 3758, 14, 3092, 8, 1324, 12, 4176, 29, 24112, 9, 241, 302, 5715, 1614, 426, 12, 655, 527, 9, 298, 5150, 814, 324, 612, 233, 5715, 814, 324, 612, 222, 5150, 814, 645, 612, 233, 5715, 814, 645, 612, 222, 5150, 814, 1276, 612, 233, 272, 337, 716, 63, 1276, 8, 4072, 814, 1276, 3485, 298, 302, 5150, 312, 509, 409, 716, 63, 1276, 8, 248, 12, 7960, 44863, 274, 222, 643, 1276, 233, 333, 25354, 2, 222, 264, 272, 14, 2196, 26, 241, 643, 1276, 233, 2343, 83, 26, 446, 83, 2, 446, 308, 1168, 1276, 12, 272, 14, 2196, 9, 222, 264, 7960, 44863, 26, 241, 643, 1276, 233, 2343, 83], [60, 78, 60, 78, 5, 83, 2, 446, 308, 1168, 1276, 12, 7960, 44863, 9, 222, 302, 643, 1276, 4391, 173, 295, 4724, 63, 1813, 8, 56, 12, 2185, 274, 232, 290, 1798, 231, 2185, 947, 300, 4724, 292, 811, 517, 1247, 14, 312, 1174, 232, 1195, 232, 1247, 310, 420, 783, 13, 1696, 12, 5721, 1848, 93, 222, 1802, 517, 947, 292, 2937, 2185, 14, 312, 2185, 26, 960, 222, 16334, 292, 371, 958, 517, 1247, 14, 312, 734, 232, 1513, 222, 2185, 232, 290, 232, 2185, 233, 635, 14, 18317, 8, 1813, 9, 232, 264, 635, 14, 23198, 8, 1813, 14, 2046, 12, 635, 14, 384, 274, 222, 302, 2185, 312, 264, 1781, 8, 56, 12, 333, 22816, 1979, 222, 2329, 233, 635, 14, 4419, 8], [1813, 14, 1028, 59, 16, 535, 222, 2185, 233, 2329, 59, 1813, 61, 232, 302, 2185, 4391, 173, 295, 11494, 1614, 7194, 12, 655, 1440, 274, 232, 290, 1407, 642, 3796, 385, 5721, 8542, 253, 231, 7638, 3630, 312, 507, 713, 8387, 14635, 981, 2232, 311, 256, 38642, 232, 12482, 14, 312, 1174, 232, 1195, 232, 40038, 7194, 64, 310, 1969, 311, 3796, 385, 4985, 14, 6322, 8542, 461, 1496, 1593, 59, 16, 61, 312, 2330, 310, 2861, 12, 623, 585, 713, 222, 17718, 19316, 461, 7436, 14, 647, 693, 12, 551, 664, 3677, 222, 308, 23418, 9, 2280, 19740, 14, 312, 252, 63, 2461, 310, 698, 12, 377, 585, 713, 222, 3482, 311, 2751, 292, 2665, 14, 647, 2541, 292, 377, 551, 300, 222, 4322, 562, 292], [256, 1165, 3300, 311, 256, 3796, 14, 312, 2280, 63, 977, 310, 698, 385, 30288, 942, 222, 15103, 256, 48887, 296, 49400, 6399, 14, 312, 734, 232, 1513, 232, 7187, 311, 18407, 10999, 311, 256, 4962, 14, 507, 2405, 3796, 602, 232, 339, 9622, 301, 14, 312, 3142, 232, 1009, 232, 2296, 300, 2738, 292, 9413, 5721, 350, 12461, 3796, 253, 256, 1496, 1052, 1125, 2914, 705, 1247, 233, 4326, 17, 1995, 443, 9449, 404, 18, 1995, 396, 9449, 404, 16, 1995, 443, 29689, 558, 705, 522, 233, 635, 14, 783, 929, 16, 12, 396, 12, 554, 535, 2914, 705, 497, 4985, 14, 6322, 978, 23579, 63, 2271, 558, 705, 1247, 63, 6322, 233, 23579, 63, 2271, 8, 56, 9, 2914, 705, 497, 15673, 14, 1377, 978, 11494], [558, 705, 1247, 12, 1247, 63, 6322, 12, 522, 233, 11494, 8, 56, 12, 1247, 63, 6322, 12, 522, 12, 2280, 63, 977, 29, 16, 9, 558, 705, 1247, 558, 960, 5773, 396, 1995, 179, 443, 9449, 1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705, 1247, 63, 6322, 7700, 294, 7844, 26, 382, 18526, 382, 23984, 63, 22062, 558, 659, 19, 88, 18, 5721, 1848, 311, 630, 4755, 1653, 269, 2786, 14, 1345, 1508, 7, 5924, 880, 461, 1163, 3394, 2347, 253, 2705, 3791, 16662, 11072, 1115, 30, 2914, 705, 1247, 63, 6322, 14, 22816, 323, 558, 960, 5773, 396, 1995, 179, 443, 9449, 1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705], [522, 558, 960, 929, 16, 12, 396, 12, 443, 535, 2914, 705, 11494, 8, 89, 12, 252, 63, 2461, 29, 18, 12, 2280, 63, 977, 29, 16, 9, 558, 960, 929, 16, 12, 396, 535, 4601, 2206, 2091, 232, 1009, 232, 310, 692, 749, 24661, 14, 6571, 63, 6436, 14, 35270, 64, 232, 310, 1117, 749, 24661, 14, 1377, 14, 11907, 64, 232, 290, 232, 2280, 63, 977, 233, 951, 63, 2437, 63, 977, 8, 1440, 14, 1331, 359, 2437, 63, 977, 340, 377, 353, 232, 2330, 233, 1401, 14, 1331, 359, 1510, 340, 623, 9, 232, 901, 63, 78, 63, 2461, 233, 1401, 14, 1331, 359, 78, 63, 2461, 340, 377, 9, 232, 264, 1401, 26, 222, 510, 911, 439, 8926, 3718, 1445, 26, 446, 82, 2], [446, 1401, 14, 1077, 1002, 312, 264, 553, 8, 7194, 9, 451, 443, 26, 222, 302, 377, 312, 1165, 233, 3796, 59, 16, 61, 232, 252, 63, 2461, 233, 1165, 14, 1028, 59, 16, 61, 264, 1781, 8, 2387, 12, 269, 1028, 397, 425, 553, 8, 2387, 9, 312, 264, 901, 63, 78, 63, 2461, 300, 377, 26, 222, 901, 63, 78, 63, 2461, 233, 252, 63, 2461, 312, 264, 901, 63, 78, 63, 2461, 751, 252, 63, 2461, 26, 222, 510, 911, 439, 4453, 1945, 446, 68, 555, 311, 3796, 461, 1805, 446, 68, 2, 446, 308, 241, 901, 63, 78, 63, 2461, 12, 252, 63, 2461, 353, 312, 3796, 233, 951, 63, 7194, 1614, 7194, 12, 5721, 63, 514, 567, 9422, 397, 312, 264, 2330, 26], [222, 2784, 233, 2280, 63, 977, 14, 8427, 8, 16, 12, 252, 63, 2461, 12, 1208, 2471, 852, 63, 78, 63, 2461, 5154, 232, 425, 26, 222, 2784, 233, 635, 14, 4419, 8, 78, 63, 2461, 9, 222, 2280, 63, 977, 14, 11907, 8, 2662, 9, 222, 2784, 233, 2784, 1018, 852, 63, 78, 63, 2461, 61, 312, 18407, 63, 7194, 233, 787, 312, 296, 960, 253, 3796, 26, 222, 960, 233, 960, 59, 2662, 61, 222, 18407, 63, 7194, 14, 576, 8, 783, 9, 312, 264, 553, 8, 26911, 63, 7194, 9, 451, 396, 26, 222, 294, 30455, 259, 35516, 296, 256, 2593, 1041, 1539, 222, 302, 18407, 63, 7194, 59, 16, 61, 232, 425, 26, 222, 302, 18407, 63, 7194, 4391, 173, 295, 11226, 1614, 7194], [12, 655, 1440, 274, 232, 290, 2885, 7313, 3796, 385, 5721, 8542, 253, 231, 7638, 3630, 312, 841, 300, 231, 11295, 3980, 292, 581, 11748, 1614, 7194, 12, 2330, 29, 722, 6414, 292, 764, 232, 2280, 19740, 311, 256, 4962, 14, 312, 1174, 232, 1195, 232, 40038, 7194, 64, 310, 1969, 311, 3796, 385, 4985, 14, 6322, 8542, 461, 1496, 1593, 59, 16, 61, 312, 2280, 63, 977, 310, 698, 385, 30288, 942, 222, 15103, 256, 48887, 296, 49400, 6399, 14, 312, 252, 63, 2461, 310, 698, 12, 377, 585, 713, 222, 3482, 311, 2751, 292, 2665, 14, 647, 2541, 292, 377, 551, 300, 222, 4322, 562, 292, 256, 1165, 3300, 311, 256, 3796, 14, 312, 734, 232, 1513, 232, 7187, 311, 31575, 10999, 311, 256, 4962, 14], [507, 2405, 3796, 602, 232, 339, 9622, 301, 14, 312, 3142, 232, 1009, 232, 2296, 300, 2738, 292, 9413, 5721, 350, 12461, 3796, 253, 256, 1496, 1052, 1125, 2914, 705, 1247, 233, 4326, 17, 1995, 443, 9449, 404, 18, 1995, 396, 9449, 404, 16, 1995, 443, 29689, 558, 705, 522, 233, 635, 14, 783, 929, 16, 12, 396, 12, 554, 535, 2914, 705, 497, 4985, 14, 6322, 978, 23579, 63, 2271, 558, 705, 1247, 63, 6322, 233, 23579, 63, 2271, 8, 56, 9, 2914, 705, 497, 15673, 14, 1377, 978, 11226, 558, 705, 1247, 12, 1247, 63, 6322, 12, 522, 233, 11226, 8, 56, 12, 1247, 63, 6322, 12, 522, 12, 2280, 63, 977, 29, 16, 9, 558, 705, 1247, 558, 960, 5773, 443, 1995, 179, 443, 9449], [1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705, 1247, 63, 6322, 7700, 294, 7844, 26, 382, 18526, 382, 23984, 63, 22062, 558, 659, 19, 88, 18, 5721, 1848, 311, 630, 4755, 1653, 269, 2786, 14, 1345, 1508, 7, 5924, 880, 461, 869, 3394, 2347, 253, 2705, 3791, 16662, 11072, 1115, 30, 2914, 705, 1247, 63, 6322, 14, 22816, 323, 558, 960, 5773, 443, 1995, 179, 443, 9449, 1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705, 522, 558, 960, 929, 18, 12, 396, 12, 443, 535, 2914, 705, 11226, 8, 89, 12, 252, 63, 2461, 29, 18, 12, 2280, 63, 977, 29, 16, 9, 558, 960, 929, 16, 12, 396, 535, 312, 2206], [2091, 232, 1009, 232, 310, 1117, 749, 24661, 14, 1377, 14, 11748, 64, 232, 290, 232, 1401, 357, 1510, 358, 233, 693, 232, 302, 11494, 1614, 7194, 12, 655, 1440, 9, 4391, 173, 295, 4724, 63, 25669, 8, 56, 12, 1875, 29, 678, 274, 232, 290, 1611, 34080, 38030, 8658, 311, 960, 13, 24183, 350, 5721, 8542, 14, 312, 1174, 232, 1195, 232, 1247, 310, 960, 2154, 12, 1848, 12, 5721, 1848, 312, 734, 232, 1513, 232, 1247, 655, 554, 310, 1166, 34080, 7912, 232, 290, 232, 1247, 233, 4724, 63, 5293, 8, 56, 9, 232, 264, 29638, 8, 56, 274, 222, 264, 1875, 26, 241, 1247, 233, 1247, 14, 1556, 323, 222, 1247, 14, 441, 655, 29, 554, 232, 425, 26, 222, 264, 1875, 26, 241, 1247], [233, 1247, 655, 554, 222, 425, 26, 241, 1247, 655, 29, 554, 232, 302, 1247, 4391, 173, 295, 2055, 63, 13798, 63, 9352, 8, 78, 12, 252, 63, 34671, 274, 232, 290, 8408, 292, 969, 252, 63, 34671, 9301, 7495, 1129, 292, 252, 14, 312, 3142, 232, 1009, 232, 705, 497, 15673, 14, 1377, 978, 2055, 63, 13798, 63, 9352, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 396, 353, 232, 404, 3741, 8, 16, 12, 2009, 12, 377, 1460, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 2009, 353, 9774, 294, 23143, 26, 382, 18526, 232, 404, 3741, 8, 16, 12, 396, 12, 377, 491, 3905, 8, 17, 12, 554, 12, 377, 491, 9640, 3905, 8, 25, 12, 2009], [12, 377, 1460, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 1462, 353, 9853, 294, 23143, 26, 382, 18526, 232, 404, 3741, 8, 16, 12, 554, 12, 377, 491, 3905, 8, 18, 12, 1163, 12, 377, 491, 9640, 3905, 8, 24, 12, 2009, 12, 377, 1460, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 869, 353, 232, 404, 3741, 8, 16, 12, 1163, 12, 377, 491, 3905, 8, 20, 12, 2624, 12, 377, 491, 3905, 8, 23, 12, 2009, 12, 377, 1460, 232, 290, 232, 888, 233, 443, 232, 296, 5435, 63, 810, 253, 1004, 8, 78, 63, 34671, 274, 222, 551, 63, 78, 233, 252, 3864, 252, 63, 34671, 222, 264, 5435, 63, 810, 659, 252, 446, 252, 63], [34671, 26, 241, 551, 63, 78, 793, 396, 222, 264, 551, 63, 78, 751, 443, 26, 241, 974, 233, 888, 382, 551, 63, 78, 241, 1483, 3905, 8, 776, 12, 974, 12, 377, 9, 241, 888, 233, 974, 4391, 173, 295, 292, 2823, 8, 88, 274, 232, 290, 27640, 3209, 548, 292, 231, 7187, 12, 29289, 231, 1875, 264, 2738, 881, 232, 264, 735, 8, 88, 12, 635, 14, 2527, 274, 222, 302, 635, 14, 5293, 8, 88, 9, 232, 639, 735, 8, 88, 12, 7187, 274, 222, 302, 548, 232, 425, 26, 222, 302, 495, 8, 88, 9, 4391, 173, 692, 44910, 4491, 8, 4491, 274, 232, 333, 9763, 2671, 292, 8398, 12169, 7316, 2, 173], [280, 173, 26713, 1750, 1036, 173, 1559, 1500, 253, 626, 13, 6078, 3000, 433, 1312, 256, 17079, 39612, 18610, 308, 35, 3110, 9, 173, 26713, 1750, 1036, 173, 173, 2096, 626, 13, 6078, 15133, 300, 6977, 585, 16321, 1187, 2111, 433, 542, 173, 2264, 2085, 6015, 13, 13488, 815, 2597, 9262, 14, 841, 300, 958, 292, 3115, 231, 173, 14190, 1848, 1187, 1086, 4647, 385, 231, 19564, 626, 13, 2773, 350, 2721, 256, 447, 3110, 173, 8038, 7623, 14, 393, 5273, 38564, 831, 11459, 300, 1319, 4498, 292, 2874, 173, 10639, 14, 173, 280, 173, 3, 5764, 1266, 26, 9491, 22171, 704, 87, 7241, 3608, 659, 306, 22171, 14, 10304, 7241, 3608, 32, 9974, 85, 14, 11016, 30, 173, 3, 3708, 1367, 46733, 13, 405, 1241, 2020, 266], [659, 3497, 254, 221, 1241, 14, 6278, 32, 29024, 14, 1026, 30, 173, 3, 3708, 41336, 19414, 918, 831, 659, 238, 334, 402, 221, 14, 2007, 918, 831, 32, 29024, 14, 1026, 30, 173, 3, 3708, 41336, 402, 221, 704, 5872, 11306, 659, 238, 334, 402, 221, 14, 1866, 11306, 32, 219, 30343, 14, 7164, 30, 173, 3, 173, 3, 24384, 26, 37634, 308, 19, 13, 10492, 9, 4391, 173, 2745, 1601, 442, 635, 173, 2745, 4855, 14, 11032, 442, 2564, 173, 173, 973, 44445, 978, 26497, 83, 12, 969, 63, 666, 12, 3096, 63, 973, 63, 7140, 173, 973, 44445, 14, 1425, 978, 14885, 63, 1311, 83, 12, 1095, 63, 1311, 63, 28807, 173, 973, 44445, 14, 7511, 978, 19960, 3661, 482, 173, 973, 44445, 14, 24983], [978, 447, 3110, 173, 973, 44445, 14, 513, 63, 6078, 978, 18140, 13175, 50, 173, 173, 973, 15673, 14, 1323, 17464, 831, 63, 5188, 978, 13885, 3743, 17464, 831, 9099, 173, 973, 15673, 14, 961, 63, 5178, 978, 7894, 225, 3859, 43, 30709, 12, 5665, 63, 307, 63, 2988, 173, 973, 15673, 14, 5763, 978, 1409, 63, 5763, 173, 973, 15673, 14, 30635, 978, 9558, 11247, 173, 173, 5824, 5824, 44032, 173, 3, 1743, 1439, 350, 1095, 433, 173, 1298, 63, 293, 233, 805, 8, 72, 4276, 29, 18, 12, 36773, 29, 19, 9, 179, 294, 13486, 45849, 89, 26, 41332, 6259, 36773, 173, 4052, 233, 396, 173, 9675, 233, 404, 22, 12, 2009, 12, 6710, 61, 173, 1311, 63, 19268, 233, 19960, 3661, 482, 14, 723, 63], [441, 8, 4052, 12, 6518, 9, 173, 1311, 233, 14885, 63, 1311, 83, 929, 628, 63, 1311, 63, 28807, 8, 70, 12, 31447, 29, 678, 9, 296, 242, 253, 2111, 63, 19268, 535, 173, 173, 3, 6331, 1755, 497, 256, 2111, 480, 173, 83, 3946, 233, 2111, 14, 666, 357, 83, 3946, 358, 173, 3481, 12, 409, 233, 3096, 63, 973, 63, 7140, 8, 1311, 12, 1083, 63, 293, 29, 645, 8, 52, 17, 29, 18, 12, 329, 18, 29, 19, 353, 173, 1311, 14, 5328, 63, 1240, 8, 23699, 29, 722, 12, 262, 8428, 29, 678, 12, 13804, 29, 722, 12, 262, 1053, 29, 722, 12, 3543, 567, 5657, 83, 397, 173, 173, 3, 28232, 256, 11459, 1312, 21372, 13, 7632, 4858, 173, 16158, 233, 1409, 63], [5763, 8, 35, 3110, 8, 78, 63, 4263, 29, 20, 12, 1060, 29, 325, 12, 559, 29, 678, 12, 4365, 63, 3281, 29, 722, 491, 370, 13885, 3743, 17464, 831, 9099, 1002, 173, 78, 63, 9779, 233, 1462, 179, 294, 3048, 3729, 26513, 292, 811, 296, 5665, 13, 6436, 173, 7549, 233, 7894, 225, 3859, 43, 30709, 8, 78, 63, 9779, 29, 78, 63, 9779, 12, 11226, 29, 678, 9, 173, 173, 3, 34182, 1874, 3692, 13, 6078, 1439, 173, 22976, 12, 20785, 233, 21345, 3865, 12, 554, 14, 1811, 173, 78, 63, 9952, 233, 2009, 14, 179, 294, 3048, 3729, 3478, 12036, 26, 958, 292, 5294, 2535, 1208, 173, 728, 63, 3946, 233, 1462, 14, 173, 852, 63, 3946, 233, 7088, 14, 173, 78, 63, 13838, 233], [2070, 179, 294, 3048, 3729, 4281, 4906, 292, 811, 173, 173, 3, 28232, 495, 311, 4281, 1004, 4193, 173, 13838, 233, 635, 14, 8042, 8, 728, 63, 3946, 12, 901, 63, 3946, 12, 252, 63, 13838, 9, 179, 294, 15656, 9262, 173, 3946, 63, 6568, 233, 495, 8, 2448, 8, 13838, 3022, 17, 547, 16384, 59, 17, 12123, 179, 294, 1409, 16384, 495, 311, 4193, 173, 173, 3, 31585, 2535, 9677, 497, 256, 901, 4945, 350, 936, 311, 12036, 292, 4227, 12349, 173, 2773, 63, 9005, 233, 308, 78, 63, 9952, 823, 635, 14, 852, 8, 13838, 9, 823, 554, 3577, 173, 18940, 63, 87, 63, 3467, 233, 635, 14, 4419, 8, 22976, 12, 20785, 12, 2535, 63, 9005, 2174, 17, 2056, 173, 78, 63, 7385, 233, 553], [8, 18940, 63, 87, 63, 3467, 9, 173, 173, 3, 19038, 1215, 7291, 173, 239, 233, 9558, 11247, 323, 173, 173, 5824, 5824, 44032, 173, 3, 10082, 2727, 9262, 12, 2937, 11459, 350, 2183, 5410, 173, 173, 3, 2884, 5410, 173, 3946, 63, 5348, 233, 635, 14, 2470, 1139, 78, 63, 13838, 415, 396, 5154, 173, 173, 3, 10082, 2727, 1086, 4281, 1004, 311, 7536, 173, 1000, 4945, 12, 308, 25786, 12, 28583, 9, 253, 2021, 8, 3946, 63, 6568, 274, 312, 294, 31585, 2535, 1208, 2249, 517, 256, 4281, 2806, 958, 232, 283, 63, 708, 233, 252, 63, 9952, 823, 3243, 31673, 382, 22128, 9, 823, 554, 3577, 179, 294, 253, 2565, 312, 294, 8092, 6015, 13, 1685, 1336, 292, 31146, 256, 1133, 9262, 232, 2111, 63], [1250, 233, 2111, 14, 1556, 1239, 1250, 8, 25786, 12, 28583, 12, 252, 63, 3475, 29, 17, 12, 27022, 63, 9838, 567, 8994, 3807, 340, 2935, 2648, 63, 876, 63, 6603, 567, 2952, 397, 312, 294, 6331, 13400, 497, 4911, 433, 12, 9885, 585, 2535, 1208, 232, 13400, 233, 26497, 83, 8, 1311, 63, 1250, 12, 3096, 12, 1083, 63, 293, 12, 26395, 415, 283, 63, 708, 12, 20785, 382, 283, 63, 708, 12, 370, 8412, 29, 722, 12, 11095, 29, 325, 12, 31447, 29, 678, 9, 232, 13400, 14, 4252, 63, 5657, 323, 232, 522, 233, 821, 14, 2770, 63, 2972, 8, 11193, 14, 3481, 1853, 554, 535, 312, 1247, 233, 13400, 14, 322, 63, 441, 323, 312, 294, 6796, 2600, 5410, 1187, 26513, 296, 1086, 4281], [350, 626, 2535, 232, 4945, 63, 5348, 59, 3946, 61, 233, 635, 14, 2257, 8, 6571, 63, 307, 63, 2988, 8, 9411, 29, 16158, 12, 1247, 29, 56, 12, 522, 29, 89, 12, 4899, 16913, 567, 18714, 63, 21311, 340, 7240, 29, 7549, 12, 4899, 252, 63, 3475, 29, 17, 491, 1612, 29, 16, 9, 173, 173, 5824, 5824, 44032, 173, 3, 6959, 4281, 1354, 173, 173, 8436, 14, 2007, 8, 13838, 3022, 17, 547, 4945, 63, 5348, 12, 2246, 29, 1075, 14, 2293, 8, 13838, 2174, 16, 547, 222, 3213, 567, 2952, 340, 19392, 567, 6127, 397, 173, 8436, 14, 14423, 8, 13838, 9, 173, 8436, 14, 8093, 929, 16, 12, 396, 535, 173, 8436, 14, 29385, 8, 563, 8, 11193, 357, 717, 444, 1191, 823, 553], [8, 11193, 491, 1639, 567, 75, 340, 15042, 46686, 241, 1215, 567, 38038, 1764, 397, 173, 8436, 14, 6405, 323, 173, 8436, 14, 6996, 359, 17017, 308, 12533, 6546, 173, 8436, 14, 6988, 359, 1559, 1500, 438, 2829, 397, 173, 8436, 14, 1687, 359, 17017, 46902, 438, 2829, 397, 173, 173, 5824, 5824, 44032, 173, 3, 10082, 2727, 9262, 350, 626, 12, 2937, 11459, 350, 2183, 5410, 173, 173, 3, 2884, 5410, 173, 2932, 63, 5348, 233, 635, 14, 2470, 1139, 78, 63, 13838, 415, 396, 12, 252, 63, 7385, 353, 173, 173, 3, 10082, 2727, 1086, 4281, 1004, 311, 7536, 173, 1000, 4945, 12, 308, 25786, 12, 28583, 9, 253, 2021, 8, 3946, 63, 6568, 274, 312, 294, 31585, 2535, 1208, 2249, 517, 256, 4281, 2806, 958], [232, 283, 63, 708, 233, 252, 63, 9952, 823, 3243, 31673, 382, 22128, 9, 823, 554, 3577, 179, 294, 253, 2565, 312, 294, 8092, 6015, 13, 1685, 1336, 292, 31146, 256, 1133, 9262, 232, 2111, 63, 1250, 233, 2111, 14, 1556, 1239, 1250, 8, 25786, 12, 28583, 12, 252, 63, 3475, 29, 17, 12, 27022, 63, 9838, 567, 8994, 3807, 340, 2935, 2648, 63, 876, 63, 6603, 567, 2952, 397, 312, 294, 6331, 13400, 497, 4911, 433, 12, 9885, 585, 2535, 1208, 232, 13400, 233, 26497, 83, 8, 1311, 63, 1250, 12, 3096, 12, 1083, 63, 293, 12, 26395, 415, 283, 63, 708, 12, 20785, 382, 283, 63, 708, 12, 370, 8412, 29, 722, 12, 11095, 29, 325, 12, 31447, 29, 678, 9, 232, 13400, 14, 4252, 63], [5657, 323, 232, 522, 233, 821, 14, 2770, 63, 2972, 8, 11193, 14, 3481, 1853, 554, 535, 312, 294, 26674, 7548, 12, 243, 398, 350, 33450, 1187, 626, 232, 296, 226, 12, 283, 63, 513, 253, 2021, 8, 18940, 63, 87, 63, 3467, 274, 298, 294, 23106, 256, 1074, 350, 901, 311, 256, 2535, 222, 283, 63, 22976, 233, 283, 63, 513, 415, 283, 63, 708, 823, 554, 14, 222, 283, 63, 21615, 233, 283, 63, 513, 382, 283, 63, 708, 823, 554, 14, 298, 294, 41640, 433, 1202, 626, 13, 2773, 311, 7536, 222, 1247, 233, 13400, 14, 1556, 1239, 9027, 8, 87, 63, 22976, 12, 283, 63, 21615, 625, 322, 63, 441, 323, 298, 294, 6796, 2600, 5410, 1187, 26513, 296, 1086, 4281, 350, 626, 2535], [222, 1784, 63, 5348, 59, 3946, 12, 226, 61, 233, 635, 14, 2257, 8, 6571, 63, 307, 63, 2988, 8, 9411, 29, 16158, 12, 1247, 29, 56, 12, 522, 29, 89, 12, 7089, 16913, 567, 18714, 63, 21311, 340, 7240, 29, 7549, 12, 7089, 252, 63, 3475, 29, 17, 491, 1612, 29, 16, 9, 173, 173, 5824, 5824, 44032, 173, 3, 6959, 626, 13, 6078, 1354, 173, 173, 3, 1743, 1129, 626, 4281, 624, 173, 1204, 63, 2932, 82, 233, 18140, 13175, 50, 8, 1244, 63, 666, 3411, 3946, 1090, 259, 3946, 491, 1784, 63, 5348, 59, 1075, 14, 9237, 12, 14599, 370, 15535, 63, 87, 63, 3467, 12, 16384, 59, 17, 6640, 396, 9, 173, 173, 38038, 233, 635, 14, 2257, 8, 89, 9, 179, 294, 562], [17813, 1764, 292, 7216, 253, 256, 1676, 173, 1204, 63, 2932, 82, 14, 1389, 929, 16, 547, 10809, 29, 38038, 12, 2081, 861, 1932, 13, 17017, 46902, 438, 2829, 485, 241, 6677, 29, 8436, 14, 5248, 14, 5783, 83, 9, 173]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'length': [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b58e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 34\n",
      "Input chunk length: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk length: {outputs['length']}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "036ff3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': [[280, 173, 2096, 310, 2025, 749, 24661, 14, 1377, 64, 1340, 6376, 8258, 4705, 2646, 14, 173, 280, 173, 173, 973, 4962, 978, 7187, 173, 173, 2745, 1601, 442, 635, 173, 973, 4985, 14, 6322, 978, 29638, 173, 2745, 3758, 173, 173, 973, 1428, 77, 26853, 250, 1817, 978, 288, 26853, 250, 1817, 19, 63, 1551, 173, 973, 1428, 6436, 978, 308, 352, 63, 1345, 63, 783, 12, 951, 63, 7194, 12, 4724, 63, 5293, 12, 373, 1387, 63, 536, 63, 7544, 12, 960, 18, 68, 12, 35001, 18, 68, 63, 228, 63, 22229, 12, 373, 35001, 18, 68, 63, 228, 63, 9422, 12, 6792, 63, 804, 63, 854, 63, 1345, 12, 373, 951, 63, 2437, 63, 977, 9, 173, 973, 1428, 692, 63, 2077, 978, 3115, 63], [692, 63, 2077, 173, 173, 612, 536, 612, 233, 2558, 77, 26853, 250, 1817, 19, 63, 1551, 485, 333, 352, 63, 1345, 63, 783, 485, 333, 1207, 63, 7194, 485, 333, 4048, 63, 5293, 485, 1366, 333, 5224, 63, 536, 63, 7544, 485, 333, 783, 18, 68, 485, 333, 11198, 18, 68, 63, 228, 63, 22229, 485, 1366, 333, 11198, 18, 68, 63, 228, 63, 9422, 485, 333, 3092, 63, 804, 63, 854, 63, 1345, 485, 333, 1207, 63, 2437, 63, 977, 485, 1366, 333, 4180, 63, 692, 63, 2077, 778, 173, 173, 3, 3596, 2926, 542, 12925, 549, 7784, 173, 7150, 14, 21848, 439, 10353, 485, 12925, 9, 4391, 173, 692, 5580, 8, 1023, 274, 232, 290, 12392, 292, 2391, 231, 753, 385, 1149, 442, 5580, 14], [312, 16376, 231, 2671, 1184, 256, 753, 300, 2072, 15, 1941, 1149, 300, 14200, 350, 232, 7078, 231, 2671, 292, 256, 5896, 14, 312, 507, 1154, 1480, 1041, 664, 371, 8415, 292, 256, 19475, 952, 232, 350, 256, 5896, 14, 2321, 26, 292, 811, 551, 461, 256, 713, 448, 296, 1480, 12, 4124, 232, 253, 309, 2039, 311, 26347, 26, 312, 705, 497, 15673, 14, 1377, 978, 5580, 232, 705, 5580, 323, 294, 7844, 26, 382, 18526, 232, 659, 24661, 14, 1377, 14, 11948, 624, 815, 44206, 312, 705, 1111, 11948, 323, 232, 1329, 509, 1791, 63, 1619, 794, 989, 232, 290, 312, 294, 25923, 497, 2297, 1193, 5184, 14, 2580, 14, 2129, 15, 607, 219, 15, 6545, 12392, 12532, 12, 232, 294, 1314, 461, 3729, 3099, 14], [312, 509, 1289, 1863, 4223, 248, 12, 1480, 7967, 222, 290, 222, 1174, 222, 1195, 222, 1480, 26, 671, 880, 292, 371, 2621, 292, 256, 19475, 3039, 298, 290, 222, 272, 14, 2196, 233, 1480, 312, 509, 1289, 1096, 4223, 248, 12, 1300, 274, 222, 264, 735, 8, 886, 12, 630, 274, 241, 302, 272, 337, 22733, 63, 692, 8, 886, 9, 222, 425, 26, 241, 302, 272, 337, 22733, 63, 4072, 8, 886, 9, 312, 509, 409, 22733, 63, 692, 8, 248, 12, 1429, 274, 222, 1347, 233, 333, 2425, 446, 83, 300, 5580, 2, 446, 1429, 814, 324, 612, 222, 264, 272, 14, 2196, 26, 241, 1347, 793, 20357, 446, 83, 2, 446, 272, 14, 2196, 298, 294, 7469, 26, 649, 1037, 7125, 4096, 1289, 1168], [612, 296, 2026, 1251, 4238, 222, 2884, 233, 1429, 814, 1863, 612, 298, 509, 5150, 1614, 426, 12, 655, 527, 274, 241, 3758, 14, 3092, 8, 1324, 12, 4176, 29, 24112, 9, 241, 302, 2884, 1614, 426, 12, 655, 527, 9, 222, 1429, 814, 1863, 612, 233, 5150, 298, 5150, 814, 324, 612, 233, 4488, 1863, 14055, 222, 5150, 814, 1276, 612, 233, 272, 337, 716, 63, 1276, 8, 1863, 814, 1276, 3485, 222, 5150, 14, 11948, 63, 4884, 233, 2884, 298, 302, 1429, 312, 509, 409, 22733, 63, 4072, 8, 248, 12, 5715, 274, 222, 290, 33531, 753, 5715, 280, 298, 1347, 233, 333, 4680, 446, 83, 300, 5580, 2, 446, 5715, 814, 324, 612, 222, 264, 272, 14, 2196, 26, 241, 1347, 793, 20357, 446, 83], [2, 446, 272, 14, 2196, 298, 509, 5150, 1614, 426, 12, 655, 527, 274, 241, 3758, 14, 3092, 8, 1324, 12, 4176, 29, 24112, 9, 241, 302, 5715, 1614, 426, 12, 655, 527, 9, 298, 5150, 814, 324, 612, 233, 5715, 814, 324, 612, 222, 5150, 814, 645, 612, 233, 5715, 814, 645, 612, 222, 5150, 814, 1276, 612, 233, 272, 337, 716, 63, 1276, 8, 4072, 814, 1276, 3485, 298, 302, 5150, 312, 509, 409, 716, 63, 1276, 8, 248, 12, 7960, 44863, 274, 222, 643, 1276, 233, 333, 25354, 2, 222, 264, 272, 14, 2196, 26, 241, 643, 1276, 233, 2343, 83, 26, 446, 83, 2, 446, 308, 1168, 1276, 12, 272, 14, 2196, 9, 222, 264, 7960, 44863, 26, 241, 643, 1276, 233, 2343, 83], [60, 78, 60, 78, 5, 83, 2, 446, 308, 1168, 1276, 12, 7960, 44863, 9, 222, 302, 643, 1276, 4391, 173, 295, 4724, 63, 1813, 8, 56, 12, 2185, 274, 232, 290, 1798, 231, 2185, 947, 300, 4724, 292, 811, 517, 1247, 14, 312, 1174, 232, 1195, 232, 1247, 310, 420, 783, 13, 1696, 12, 5721, 1848, 93, 222, 1802, 517, 947, 292, 2937, 2185, 14, 312, 2185, 26, 960, 222, 16334, 292, 371, 958, 517, 1247, 14, 312, 734, 232, 1513, 222, 2185, 232, 290, 232, 2185, 233, 635, 14, 18317, 8, 1813, 9, 232, 264, 635, 14, 23198, 8, 1813, 14, 2046, 12, 635, 14, 384, 274, 222, 302, 2185, 312, 264, 1781, 8, 56, 12, 333, 22816, 1979, 222, 2329, 233, 635, 14, 4419, 8], [1813, 14, 1028, 59, 16, 535, 222, 2185, 233, 2329, 59, 1813, 61, 232, 302, 2185, 4391, 173, 295, 11494, 1614, 7194, 12, 655, 1440, 274, 232, 290, 1407, 642, 3796, 385, 5721, 8542, 253, 231, 7638, 3630, 312, 507, 713, 8387, 14635, 981, 2232, 311, 256, 38642, 232, 12482, 14, 312, 1174, 232, 1195, 232, 40038, 7194, 64, 310, 1969, 311, 3796, 385, 4985, 14, 6322, 8542, 461, 1496, 1593, 59, 16, 61, 312, 2330, 310, 2861, 12, 623, 585, 713, 222, 17718, 19316, 461, 7436, 14, 647, 693, 12, 551, 664, 3677, 222, 308, 23418, 9, 2280, 19740, 14, 312, 252, 63, 2461, 310, 698, 12, 377, 585, 713, 222, 3482, 311, 2751, 292, 2665, 14, 647, 2541, 292, 377, 551, 300, 222, 4322, 562, 292], [256, 1165, 3300, 311, 256, 3796, 14, 312, 2280, 63, 977, 310, 698, 385, 30288, 942, 222, 15103, 256, 48887, 296, 49400, 6399, 14, 312, 734, 232, 1513, 232, 7187, 311, 18407, 10999, 311, 256, 4962, 14, 507, 2405, 3796, 602, 232, 339, 9622, 301, 14, 312, 3142, 232, 1009, 232, 2296, 300, 2738, 292, 9413, 5721, 350, 12461, 3796, 253, 256, 1496, 1052, 1125, 2914, 705, 1247, 233, 4326, 17, 1995, 443, 9449, 404, 18, 1995, 396, 9449, 404, 16, 1995, 443, 29689, 558, 705, 522, 233, 635, 14, 783, 929, 16, 12, 396, 12, 554, 535, 2914, 705, 497, 4985, 14, 6322, 978, 23579, 63, 2271, 558, 705, 1247, 63, 6322, 233, 23579, 63, 2271, 8, 56, 9, 2914, 705, 497, 15673, 14, 1377, 978, 11494], [558, 705, 1247, 12, 1247, 63, 6322, 12, 522, 233, 11494, 8, 56, 12, 1247, 63, 6322, 12, 522, 12, 2280, 63, 977, 29, 16, 9, 558, 705, 1247, 558, 960, 5773, 396, 1995, 179, 443, 9449, 1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705, 1247, 63, 6322, 7700, 294, 7844, 26, 382, 18526, 382, 23984, 63, 22062, 558, 659, 19, 88, 18, 5721, 1848, 311, 630, 4755, 1653, 269, 2786, 14, 1345, 1508, 7, 5924, 880, 461, 1163, 3394, 2347, 253, 2705, 3791, 16662, 11072, 1115, 30, 2914, 705, 1247, 63, 6322, 14, 22816, 323, 558, 960, 5773, 396, 1995, 179, 443, 9449, 1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705], [522, 558, 960, 929, 16, 12, 396, 12, 443, 535, 2914, 705, 11494, 8, 89, 12, 252, 63, 2461, 29, 18, 12, 2280, 63, 977, 29, 16, 9, 558, 960, 929, 16, 12, 396, 535, 4601, 2206, 2091, 232, 1009, 232, 310, 692, 749, 24661, 14, 6571, 63, 6436, 14, 35270, 64, 232, 310, 1117, 749, 24661, 14, 1377, 14, 11907, 64, 232, 290, 232, 2280, 63, 977, 233, 951, 63, 2437, 63, 977, 8, 1440, 14, 1331, 359, 2437, 63, 977, 340, 377, 353, 232, 2330, 233, 1401, 14, 1331, 359, 1510, 340, 623, 9, 232, 901, 63, 78, 63, 2461, 233, 1401, 14, 1331, 359, 78, 63, 2461, 340, 377, 9, 232, 264, 1401, 26, 222, 510, 911, 439, 8926, 3718, 1445, 26, 446, 82, 2], [446, 1401, 14, 1077, 1002, 312, 264, 553, 8, 7194, 9, 451, 443, 26, 222, 302, 377, 312, 1165, 233, 3796, 59, 16, 61, 232, 252, 63, 2461, 233, 1165, 14, 1028, 59, 16, 61, 264, 1781, 8, 2387, 12, 269, 1028, 397, 425, 553, 8, 2387, 9, 312, 264, 901, 63, 78, 63, 2461, 300, 377, 26, 222, 901, 63, 78, 63, 2461, 233, 252, 63, 2461, 312, 264, 901, 63, 78, 63, 2461, 751, 252, 63, 2461, 26, 222, 510, 911, 439, 4453, 1945, 446, 68, 555, 311, 3796, 461, 1805, 446, 68, 2, 446, 308, 241, 901, 63, 78, 63, 2461, 12, 252, 63, 2461, 353, 312, 3796, 233, 951, 63, 7194, 1614, 7194, 12, 5721, 63, 514, 567, 9422, 397, 312, 264, 2330, 26], [222, 2784, 233, 2280, 63, 977, 14, 8427, 8, 16, 12, 252, 63, 2461, 12, 1208, 2471, 852, 63, 78, 63, 2461, 5154, 232, 425, 26, 222, 2784, 233, 635, 14, 4419, 8, 78, 63, 2461, 9, 222, 2280, 63, 977, 14, 11907, 8, 2662, 9, 222, 2784, 233, 2784, 1018, 852, 63, 78, 63, 2461, 61, 312, 18407, 63, 7194, 233, 787, 312, 296, 960, 253, 3796, 26, 222, 960, 233, 960, 59, 2662, 61, 222, 18407, 63, 7194, 14, 576, 8, 783, 9, 312, 264, 553, 8, 26911, 63, 7194, 9, 451, 396, 26, 222, 294, 30455, 259, 35516, 296, 256, 2593, 1041, 1539, 222, 302, 18407, 63, 7194, 59, 16, 61, 232, 425, 26, 222, 302, 18407, 63, 7194, 4391, 173, 295, 11226, 1614, 7194], [12, 655, 1440, 274, 232, 290, 2885, 7313, 3796, 385, 5721, 8542, 253, 231, 7638, 3630, 312, 841, 300, 231, 11295, 3980, 292, 581, 11748, 1614, 7194, 12, 2330, 29, 722, 6414, 292, 764, 232, 2280, 19740, 311, 256, 4962, 14, 312, 1174, 232, 1195, 232, 40038, 7194, 64, 310, 1969, 311, 3796, 385, 4985, 14, 6322, 8542, 461, 1496, 1593, 59, 16, 61, 312, 2280, 63, 977, 310, 698, 385, 30288, 942, 222, 15103, 256, 48887, 296, 49400, 6399, 14, 312, 252, 63, 2461, 310, 698, 12, 377, 585, 713, 222, 3482, 311, 2751, 292, 2665, 14, 647, 2541, 292, 377, 551, 300, 222, 4322, 562, 292, 256, 1165, 3300, 311, 256, 3796, 14, 312, 734, 232, 1513, 232, 7187, 311, 31575, 10999, 311, 256, 4962, 14], [507, 2405, 3796, 602, 232, 339, 9622, 301, 14, 312, 3142, 232, 1009, 232, 2296, 300, 2738, 292, 9413, 5721, 350, 12461, 3796, 253, 256, 1496, 1052, 1125, 2914, 705, 1247, 233, 4326, 17, 1995, 443, 9449, 404, 18, 1995, 396, 9449, 404, 16, 1995, 443, 29689, 558, 705, 522, 233, 635, 14, 783, 929, 16, 12, 396, 12, 554, 535, 2914, 705, 497, 4985, 14, 6322, 978, 23579, 63, 2271, 558, 705, 1247, 63, 6322, 233, 23579, 63, 2271, 8, 56, 9, 2914, 705, 497, 15673, 14, 1377, 978, 11226, 558, 705, 1247, 12, 1247, 63, 6322, 12, 522, 233, 11226, 8, 56, 12, 1247, 63, 6322, 12, 522, 12, 2280, 63, 977, 29, 16, 9, 558, 705, 1247, 558, 960, 5773, 443, 1995, 179, 443, 9449], [1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705, 1247, 63, 6322, 7700, 294, 7844, 26, 382, 18526, 382, 23984, 63, 22062, 558, 659, 19, 88, 18, 5721, 1848, 311, 630, 4755, 1653, 269, 2786, 14, 1345, 1508, 7, 5924, 880, 461, 869, 3394, 2347, 253, 2705, 3791, 16662, 11072, 1115, 30, 2914, 705, 1247, 63, 6322, 14, 22816, 323, 558, 960, 5773, 443, 1995, 179, 443, 9449, 1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705, 522, 558, 960, 929, 18, 12, 396, 12, 443, 535, 2914, 705, 11226, 8, 89, 12, 252, 63, 2461, 29, 18, 12, 2280, 63, 977, 29, 16, 9, 558, 960, 929, 16, 12, 396, 535, 312, 2206], [2091, 232, 1009, 232, 310, 1117, 749, 24661, 14, 1377, 14, 11748, 64, 232, 290, 232, 1401, 357, 1510, 358, 233, 693, 232, 302, 11494, 1614, 7194, 12, 655, 1440, 9, 4391, 173, 295, 4724, 63, 25669, 8, 56, 12, 1875, 29, 678, 274, 232, 290, 1611, 34080, 38030, 8658, 311, 960, 13, 24183, 350, 5721, 8542, 14, 312, 1174, 232, 1195, 232, 1247, 310, 960, 2154, 12, 1848, 12, 5721, 1848, 312, 734, 232, 1513, 232, 1247, 655, 554, 310, 1166, 34080, 7912, 232, 290, 232, 1247, 233, 4724, 63, 5293, 8, 56, 9, 232, 264, 29638, 8, 56, 274, 222, 264, 1875, 26, 241, 1247, 233, 1247, 14, 1556, 323, 222, 1247, 14, 441, 655, 29, 554, 232, 425, 26, 222, 264, 1875, 26, 241, 1247], [233, 1247, 655, 554, 222, 425, 26, 241, 1247, 655, 29, 554, 232, 302, 1247, 4391, 173, 295, 2055, 63, 13798, 63, 9352, 8, 78, 12, 252, 63, 34671, 274, 232, 290, 8408, 292, 969, 252, 63, 34671, 9301, 7495, 1129, 292, 252, 14, 312, 3142, 232, 1009, 232, 705, 497, 15673, 14, 1377, 978, 2055, 63, 13798, 63, 9352, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 396, 353, 232, 404, 3741, 8, 16, 12, 2009, 12, 377, 1460, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 2009, 353, 9774, 294, 23143, 26, 382, 18526, 232, 404, 3741, 8, 16, 12, 396, 12, 377, 491, 3905, 8, 17, 12, 554, 12, 377, 491, 9640, 3905, 8, 25, 12, 2009], [12, 377, 1460, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 1462, 353, 9853, 294, 23143, 26, 382, 18526, 232, 404, 3741, 8, 16, 12, 554, 12, 377, 491, 3905, 8, 18, 12, 1163, 12, 377, 491, 9640, 3905, 8, 24, 12, 2009, 12, 377, 1460, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 869, 353, 232, 404, 3741, 8, 16, 12, 1163, 12, 377, 491, 3905, 8, 20, 12, 2624, 12, 377, 491, 3905, 8, 23, 12, 2009, 12, 377, 1460, 232, 290, 232, 888, 233, 443, 232, 296, 5435, 63, 810, 253, 1004, 8, 78, 63, 34671, 274, 222, 551, 63, 78, 233, 252, 3864, 252, 63, 34671, 222, 264, 5435, 63, 810, 659, 252, 446, 252, 63], [34671, 26, 241, 551, 63, 78, 793, 396, 222, 264, 551, 63, 78, 751, 443, 26, 241, 974, 233, 888, 382, 551, 63, 78, 241, 1483, 3905, 8, 776, 12, 974, 12, 377, 9, 241, 888, 233, 974, 4391, 173, 295, 292, 2823, 8, 88, 274, 232, 290, 27640, 3209, 548, 292, 231, 7187, 12, 29289, 231, 1875, 264, 2738, 881, 232, 264, 735, 8, 88, 12, 635, 14, 2527, 274, 222, 302, 635, 14, 5293, 8, 88, 9, 232, 639, 735, 8, 88, 12, 7187, 274, 222, 302, 548, 232, 425, 26, 222, 302, 495, 8, 88, 9, 4391, 173, 692, 44910, 4491, 8, 4491, 274, 232, 333, 9763, 2671, 292, 8398, 12169, 7316, 2, 173], [280, 173, 26713, 1750, 1036, 173, 1559, 1500, 253, 626, 13, 6078, 3000, 433, 1312, 256, 17079, 39612, 18610, 308, 35, 3110, 9, 173, 26713, 1750, 1036, 173, 173, 2096, 626, 13, 6078, 15133, 300, 6977, 585, 16321, 1187, 2111, 433, 542, 173, 2264, 2085, 6015, 13, 13488, 815, 2597, 9262, 14, 841, 300, 958, 292, 3115, 231, 173, 14190, 1848, 1187, 1086, 4647, 385, 231, 19564, 626, 13, 2773, 350, 2721, 256, 447, 3110, 173, 8038, 7623, 14, 393, 5273, 38564, 831, 11459, 300, 1319, 4498, 292, 2874, 173, 10639, 14, 173, 280, 173, 3, 5764, 1266, 26, 9491, 22171, 704, 87, 7241, 3608, 659, 306, 22171, 14, 10304, 7241, 3608, 32, 9974, 85, 14, 11016, 30, 173, 3, 3708, 1367, 46733, 13, 405, 1241, 2020, 266], [659, 3497, 254, 221, 1241, 14, 6278, 32, 29024, 14, 1026, 30, 173, 3, 3708, 41336, 19414, 918, 831, 659, 238, 334, 402, 221, 14, 2007, 918, 831, 32, 29024, 14, 1026, 30, 173, 3, 3708, 41336, 402, 221, 704, 5872, 11306, 659, 238, 334, 402, 221, 14, 1866, 11306, 32, 219, 30343, 14, 7164, 30, 173, 3, 173, 3, 24384, 26, 37634, 308, 19, 13, 10492, 9, 4391, 173, 2745, 1601, 442, 635, 173, 2745, 4855, 14, 11032, 442, 2564, 173, 173, 973, 44445, 978, 26497, 83, 12, 969, 63, 666, 12, 3096, 63, 973, 63, 7140, 173, 973, 44445, 14, 1425, 978, 14885, 63, 1311, 83, 12, 1095, 63, 1311, 63, 28807, 173, 973, 44445, 14, 7511, 978, 19960, 3661, 482, 173, 973, 44445, 14, 24983], [978, 447, 3110, 173, 973, 44445, 14, 513, 63, 6078, 978, 18140, 13175, 50, 173, 173, 973, 15673, 14, 1323, 17464, 831, 63, 5188, 978, 13885, 3743, 17464, 831, 9099, 173, 973, 15673, 14, 961, 63, 5178, 978, 7894, 225, 3859, 43, 30709, 12, 5665, 63, 307, 63, 2988, 173, 973, 15673, 14, 5763, 978, 1409, 63, 5763, 173, 973, 15673, 14, 30635, 978, 9558, 11247, 173, 173, 5824, 5824, 44032, 173, 3, 1743, 1439, 350, 1095, 433, 173, 1298, 63, 293, 233, 805, 8, 72, 4276, 29, 18, 12, 36773, 29, 19, 9, 179, 294, 13486, 45849, 89, 26, 41332, 6259, 36773, 173, 4052, 233, 396, 173, 9675, 233, 404, 22, 12, 2009, 12, 6710, 61, 173, 1311, 63, 19268, 233, 19960, 3661, 482, 14, 723, 63], [441, 8, 4052, 12, 6518, 9, 173, 1311, 233, 14885, 63, 1311, 83, 929, 628, 63, 1311, 63, 28807, 8, 70, 12, 31447, 29, 678, 9, 296, 242, 253, 2111, 63, 19268, 535, 173, 173, 3, 6331, 1755, 497, 256, 2111, 480, 173, 83, 3946, 233, 2111, 14, 666, 357, 83, 3946, 358, 173, 3481, 12, 409, 233, 3096, 63, 973, 63, 7140, 8, 1311, 12, 1083, 63, 293, 29, 645, 8, 52, 17, 29, 18, 12, 329, 18, 29, 19, 353, 173, 1311, 14, 5328, 63, 1240, 8, 23699, 29, 722, 12, 262, 8428, 29, 678, 12, 13804, 29, 722, 12, 262, 1053, 29, 722, 12, 3543, 567, 5657, 83, 397, 173, 173, 3, 28232, 256, 11459, 1312, 21372, 13, 7632, 4858, 173, 16158, 233, 1409, 63], [5763, 8, 35, 3110, 8, 78, 63, 4263, 29, 20, 12, 1060, 29, 325, 12, 559, 29, 678, 12, 4365, 63, 3281, 29, 722, 491, 370, 13885, 3743, 17464, 831, 9099, 1002, 173, 78, 63, 9779, 233, 1462, 179, 294, 3048, 3729, 26513, 292, 811, 296, 5665, 13, 6436, 173, 7549, 233, 7894, 225, 3859, 43, 30709, 8, 78, 63, 9779, 29, 78, 63, 9779, 12, 11226, 29, 678, 9, 173, 173, 3, 34182, 1874, 3692, 13, 6078, 1439, 173, 22976, 12, 20785, 233, 21345, 3865, 12, 554, 14, 1811, 173, 78, 63, 9952, 233, 2009, 14, 179, 294, 3048, 3729, 3478, 12036, 26, 958, 292, 5294, 2535, 1208, 173, 728, 63, 3946, 233, 1462, 14, 173, 852, 63, 3946, 233, 7088, 14, 173, 78, 63, 13838, 233], [2070, 179, 294, 3048, 3729, 4281, 4906, 292, 811, 173, 173, 3, 28232, 495, 311, 4281, 1004, 4193, 173, 13838, 233, 635, 14, 8042, 8, 728, 63, 3946, 12, 901, 63, 3946, 12, 252, 63, 13838, 9, 179, 294, 15656, 9262, 173, 3946, 63, 6568, 233, 495, 8, 2448, 8, 13838, 3022, 17, 547, 16384, 59, 17, 12123, 179, 294, 1409, 16384, 495, 311, 4193, 173, 173, 3, 31585, 2535, 9677, 497, 256, 901, 4945, 350, 936, 311, 12036, 292, 4227, 12349, 173, 2773, 63, 9005, 233, 308, 78, 63, 9952, 823, 635, 14, 852, 8, 13838, 9, 823, 554, 3577, 173, 18940, 63, 87, 63, 3467, 233, 635, 14, 4419, 8, 22976, 12, 20785, 12, 2535, 63, 9005, 2174, 17, 2056, 173, 78, 63, 7385, 233, 553], [8, 18940, 63, 87, 63, 3467, 9, 173, 173, 3, 19038, 1215, 7291, 173, 239, 233, 9558, 11247, 323, 173, 173, 5824, 5824, 44032, 173, 3, 10082, 2727, 9262, 12, 2937, 11459, 350, 2183, 5410, 173, 173, 3, 2884, 5410, 173, 3946, 63, 5348, 233, 635, 14, 2470, 1139, 78, 63, 13838, 415, 396, 5154, 173, 173, 3, 10082, 2727, 1086, 4281, 1004, 311, 7536, 173, 1000, 4945, 12, 308, 25786, 12, 28583, 9, 253, 2021, 8, 3946, 63, 6568, 274, 312, 294, 31585, 2535, 1208, 2249, 517, 256, 4281, 2806, 958, 232, 283, 63, 708, 233, 252, 63, 9952, 823, 3243, 31673, 382, 22128, 9, 823, 554, 3577, 179, 294, 253, 2565, 312, 294, 8092, 6015, 13, 1685, 1336, 292, 31146, 256, 1133, 9262, 232, 2111, 63], [1250, 233, 2111, 14, 1556, 1239, 1250, 8, 25786, 12, 28583, 12, 252, 63, 3475, 29, 17, 12, 27022, 63, 9838, 567, 8994, 3807, 340, 2935, 2648, 63, 876, 63, 6603, 567, 2952, 397, 312, 294, 6331, 13400, 497, 4911, 433, 12, 9885, 585, 2535, 1208, 232, 13400, 233, 26497, 83, 8, 1311, 63, 1250, 12, 3096, 12, 1083, 63, 293, 12, 26395, 415, 283, 63, 708, 12, 20785, 382, 283, 63, 708, 12, 370, 8412, 29, 722, 12, 11095, 29, 325, 12, 31447, 29, 678, 9, 232, 13400, 14, 4252, 63, 5657, 323, 232, 522, 233, 821, 14, 2770, 63, 2972, 8, 11193, 14, 3481, 1853, 554, 535, 312, 1247, 233, 13400, 14, 322, 63, 441, 323, 312, 294, 6796, 2600, 5410, 1187, 26513, 296, 1086, 4281], [350, 626, 2535, 232, 4945, 63, 5348, 59, 3946, 61, 233, 635, 14, 2257, 8, 6571, 63, 307, 63, 2988, 8, 9411, 29, 16158, 12, 1247, 29, 56, 12, 522, 29, 89, 12, 4899, 16913, 567, 18714, 63, 21311, 340, 7240, 29, 7549, 12, 4899, 252, 63, 3475, 29, 17, 491, 1612, 29, 16, 9, 173, 173, 5824, 5824, 44032, 173, 3, 6959, 4281, 1354, 173, 173, 8436, 14, 2007, 8, 13838, 3022, 17, 547, 4945, 63, 5348, 12, 2246, 29, 1075, 14, 2293, 8, 13838, 2174, 16, 547, 222, 3213, 567, 2952, 340, 19392, 567, 6127, 397, 173, 8436, 14, 14423, 8, 13838, 9, 173, 8436, 14, 8093, 929, 16, 12, 396, 535, 173, 8436, 14, 29385, 8, 563, 8, 11193, 357, 717, 444, 1191, 823, 553], [8, 11193, 491, 1639, 567, 75, 340, 15042, 46686, 241, 1215, 567, 38038, 1764, 397, 173, 8436, 14, 6405, 323, 173, 8436, 14, 6996, 359, 17017, 308, 12533, 6546, 173, 8436, 14, 6988, 359, 1559, 1500, 438, 2829, 397, 173, 8436, 14, 1687, 359, 17017, 46902, 438, 2829, 397, 173, 173, 5824, 5824, 44032, 173, 3, 10082, 2727, 9262, 350, 626, 12, 2937, 11459, 350, 2183, 5410, 173, 173, 3, 2884, 5410, 173, 2932, 63, 5348, 233, 635, 14, 2470, 1139, 78, 63, 13838, 415, 396, 12, 252, 63, 7385, 353, 173, 173, 3, 10082, 2727, 1086, 4281, 1004, 311, 7536, 173, 1000, 4945, 12, 308, 25786, 12, 28583, 9, 253, 2021, 8, 3946, 63, 6568, 274, 312, 294, 31585, 2535, 1208, 2249, 517, 256, 4281, 2806, 958], [232, 283, 63, 708, 233, 252, 63, 9952, 823, 3243, 31673, 382, 22128, 9, 823, 554, 3577, 179, 294, 253, 2565, 312, 294, 8092, 6015, 13, 1685, 1336, 292, 31146, 256, 1133, 9262, 232, 2111, 63, 1250, 233, 2111, 14, 1556, 1239, 1250, 8, 25786, 12, 28583, 12, 252, 63, 3475, 29, 17, 12, 27022, 63, 9838, 567, 8994, 3807, 340, 2935, 2648, 63, 876, 63, 6603, 567, 2952, 397, 312, 294, 6331, 13400, 497, 4911, 433, 12, 9885, 585, 2535, 1208, 232, 13400, 233, 26497, 83, 8, 1311, 63, 1250, 12, 3096, 12, 1083, 63, 293, 12, 26395, 415, 283, 63, 708, 12, 20785, 382, 283, 63, 708, 12, 370, 8412, 29, 722, 12, 11095, 29, 325, 12, 31447, 29, 678, 9, 232, 13400, 14, 4252, 63], [5657, 323, 232, 522, 233, 821, 14, 2770, 63, 2972, 8, 11193, 14, 3481, 1853, 554, 535, 312, 294, 26674, 7548, 12, 243, 398, 350, 33450, 1187, 626, 232, 296, 226, 12, 283, 63, 513, 253, 2021, 8, 18940, 63, 87, 63, 3467, 274, 298, 294, 23106, 256, 1074, 350, 901, 311, 256, 2535, 222, 283, 63, 22976, 233, 283, 63, 513, 415, 283, 63, 708, 823, 554, 14, 222, 283, 63, 21615, 233, 283, 63, 513, 382, 283, 63, 708, 823, 554, 14, 298, 294, 41640, 433, 1202, 626, 13, 2773, 311, 7536, 222, 1247, 233, 13400, 14, 1556, 1239, 9027, 8, 87, 63, 22976, 12, 283, 63, 21615, 625, 322, 63, 441, 323, 298, 294, 6796, 2600, 5410, 1187, 26513, 296, 1086, 4281, 350, 626, 2535], [222, 1784, 63, 5348, 59, 3946, 12, 226, 61, 233, 635, 14, 2257, 8, 6571, 63, 307, 63, 2988, 8, 9411, 29, 16158, 12, 1247, 29, 56, 12, 522, 29, 89, 12, 7089, 16913, 567, 18714, 63, 21311, 340, 7240, 29, 7549, 12, 7089, 252, 63, 3475, 29, 17, 491, 1612, 29, 16, 9, 173, 173, 5824, 5824, 44032, 173, 3, 6959, 626, 13, 6078, 1354, 173, 173, 3, 1743, 1129, 626, 4281, 624, 173, 1204, 63, 2932, 82, 233, 18140, 13175, 50, 8, 1244, 63, 666, 3411, 3946, 1090, 259, 3946, 491, 1784, 63, 5348, 59, 1075, 14, 9237, 12, 14599, 370, 15535, 63, 87, 63, 3467, 12, 16384, 59, 17, 6640, 396, 9, 173, 173, 38038, 233, 635, 14, 2257, 8, 89, 9, 179, 294, 562], [17813, 1764, 292, 7216, 253, 256, 1676, 173, 1204, 63, 2932, 82, 14, 1389, 929, 16, 547, 10809, 29, 38038, 12, 2081, 861, 1932, 13, 17017, 46902, 438, 2829, 485, 241, 6677, 29, 8436, 14, 5248, 14, 5783, 83, 9, 173]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'length': [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36f47e6",
   "metadata": {},
   "source": [
    "We can see that we get 34 segments in total from those two examples. Looking at the chunk lengths, we can see that the chunks at the ends of both documents have less than 128 tokens (117 and 41, respectively). These represent just a small fraction of the total chunks that we have, so we can safely throw them away. With the `overflow_to_sample_mapping` field, we can also reconstruct which chunks belonged to which input samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5eed5",
   "metadata": {},
   "source": [
    "With this operation weâ€™re using a handy feature of the `Dataset.map()` function in ðŸ¤— Datasets, which is that it does not require one-to-one maps, we can create batches with more or fewer elements than the input batch. This is useful when doing operations like data augmentation or data filtering that change the number of elements. In our case, when tokenizing each element into chunks of the specified context size, we create many samples from each document. We just need to make sure to delete the existing columns, since they have a conflicting size. If we wanted to keep them, we could repeat them appropriately and return them within the `Dataset.map()` call:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d6897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    outputs = tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eafc0ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd981abc98246298e1c8471a57455a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/606720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenized_datasets = \u001b[43mraw_datasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenize_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m tokenized_datasets\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\datasets\\dataset_dict.py:953\u001b[39m, in \u001b[36mDatasetDict.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    951\u001b[39m     function = bind(function, split)\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m dataset_dict[split] = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    975\u001b[39m     function = function.func\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3406\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3404\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3405\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3406\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3409\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3759\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3757\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3758\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3759\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3761\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3712\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3710\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3711\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3712\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3635\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3633\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3634\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3635\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3636\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtokenize_func\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize_func\u001b[39m(examples):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     outputs = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     input_batch = []\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m length, input_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs[\u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m], outputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2456\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   2454\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_switch_to_input_mode\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2455\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2456\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2459\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2462\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2465\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_switch_to_target_mode\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\tokenization_utils_tokenizers.py:861\u001b[39m, in \u001b[36mTokenizersBackend._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    858\u001b[39m     \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001b[32m    860\u001b[39m \u001b[38;5;66;03m# Direct rust backend call\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m# Convert encodings to BatchEncoding format\u001b[39;00m\n\u001b[32m    868\u001b[39m tokens_and_encodings = [\n\u001b[32m    869\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_encoding(\n\u001b[32m    870\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    879\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    880\u001b[39m ]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_func, \n",
    "    batched=True, \n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6d7e0",
   "metadata": {},
   "source": [
    "We now have 16.7 million examples with 128 tokens each, which corresponds to about 2.1 billion tokens in total. For reference, OpenAIâ€™s GPT-3 and Codex models are trained on 300 and 100 billion tokens, respectively, where the Codex models are initialized from the GPT-3 checkpoints. Our goal in this section is not to compete with these models, which can generate long, coherent texts, but to create a scaled-down version providing a quick autocomplete function for data scientists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6489b29",
   "metadata": {},
   "source": [
    "Now that we have the dataset ready, letâ€™s set up the model!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c6bab",
   "metadata": {},
   "source": [
    "âœï¸ Try it out! Getting rid of all the chunks that are smaller than the context size wasnâ€™t a big issue here because weâ€™re using small context windows. As you increase the context size (or if you have a corpus of short documents), the fraction of chunks that are thrown away will also grow. A more efficient way to prepare the data is to join all the tokenized samples in a batch with an `eos_token_id` token in between, and then perform the chunking on the concatenated sequences. As an exercise, modify the `tokenize_func()` function to make use of that approach. Note that youâ€™ll want to set `truncation=False` and remove the other arguments from the tokenizer to get the full sequence of token IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7de96c",
   "metadata": {},
   "source": [
    "## 3. Initializing a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee35d49",
   "metadata": {},
   "source": [
    "Our first step is to freshly initialize a GPT-2 model. Weâ€™ll use the same configuration for our model as for the small GPT-2 model, so we load the pretrained configuration, make sure that the tokenizer size matches the model vocabulary size and pass the `bos` and `eos` (beginning and end of sequence) token IDs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e66c2",
   "metadata": {},
   "source": [
    "With that configuration, we can load a new model. Note that this is the first time we donâ€™t use the `from_pretrained()` function, since weâ€™re actually initializing a model ourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT2 size: {model_size / 1_000**2:.2f} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e52a68",
   "metadata": {},
   "source": [
    "Our model has 124M parameters that weâ€™ll have to tune. Before we can start training, we need to set up a data collator that will take care of creating the batches. We can use the `DataCollatorForLanguageModeling` collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels â€” in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training so we donâ€™t need to duplicate the `input_ids`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936d87e",
   "metadata": {},
   "source": [
    "Note that `DataCollatorForLanguageModeling` supports both masked language modeling (MLM) and causal language modeling (CLM). By default it prepares data for MLM, but we can switch to CLM by setting the argument `mlm=False`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be80da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ad65a",
   "metadata": {},
   "source": [
    "Letâ€™s have a look at an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7341ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [tokenized_datasets[\"train\"][i] for i in range(3)]\n",
    "out = data_collator(samples)\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6526fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172f74d",
   "metadata": {},
   "source": [
    "âš ï¸ Shifting the inputs and labels to align them happens inside the model, so the data collator just copies the inputs to create the labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef559a8",
   "metadata": {},
   "source": [
    "Now we have everything in place to actually train our model â€” that wasnâ€™t so much work after all! Before we start training we should log in to Hugging Face. If youâ€™re working in a notebook, you can do so with the following utility function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fcdf20",
   "metadata": {},
   "source": [
    "All thatâ€™s left to do is configure the training arguments and fire up the `Trainer`. Weâ€™ll use a cosine learning rate schedule with some warmup and an effective batch size of 256 (`per_device_train_batch_size * gradient_accumulation_steps`). Gradient accumulation is used when a single batch does not fit into memory, and incrementally builds up the gradient through several forward/backward passes. Weâ€™ll see this in action when we create the training loop with ðŸ¤— Accelerate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds-pretrain-gpt2\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=8,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    learning_rate=5e-4,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a8b370",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import module 'Trainer'. Are this object's requirements defined correctly?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2044\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2043\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2044\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2045\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2238\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2236\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2235\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\modeling_utils.py:86\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor_parallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     78\u001b[39m     ALL_PARALLEL_STYLES,\n\u001b[32m     79\u001b[39m     _get_parameter_tp_plan,\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m     verify_tp_plan,\n\u001b[32m     85\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_flash_attention_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lazy_import_flash_attention, lazy_import_paged_flash_attention\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_d_fine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DFineForObjectDetectionLoss\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_deformable_detr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\loss\\loss_d_fine.py:21\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_vision_available\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_for_object_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m box_iou\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_rt_detr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTDetrHungarianMatcher, RTDetrLoss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\loss\\loss_for_object_detection.py:31\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdice_loss\u001b[39m(inputs, targets, num_boxes):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\image_transforms.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     ChannelDimension,\n\u001b[32m     24\u001b[39m     ImageInput,\n\u001b[32m     25\u001b[39m     get_channel_dimension_axis,\n\u001b[32m     26\u001b[39m     get_image_size,\n\u001b[32m     27\u001b[39m     infer_channel_dimension_format,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExplicitEnum, TensorType, is_torch_tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\image_utils.py:53\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torchvision_available():\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[32m     55\u001b[39m     pil_torch_interpolation_mapping = {\n\u001b[32m     56\u001b[39m         PILImageResampling.NEAREST: InterpolationMode.NEAREST_EXACT,\n\u001b[32m     57\u001b[39m         PILImageResampling.BOX: InterpolationMode.BOX,\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m         PILImageResampling.LANCZOS: InterpolationMode.LANCZOS,\n\u001b[32m     62\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\torchvision\\_meta_registrations.py:163\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad.new_empty((batch_size, channels, height, width))\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorchvision::nms\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mmeta_nms\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mboxes should be a 2d tensor, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\library.py:1073\u001b[39m, in \u001b[36mregister_fake.<locals>.register\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m   1072\u001b[39m     use_lib = lib\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m \u001b[43muse_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_register_fake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_override\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\library.py:203\u001b[39m, in \u001b[36mLibrary._register_fake\u001b[39m\u001b[34m(self, op_name, fn, _stacklevel, allow_override)\u001b[39m\n\u001b[32m    201\u001b[39m     func_to_register = fn\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m handle = \u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfake_impl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_to_register\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_override\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m._registration_handles.append(handle)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\_library\\fake_impl.py:50\u001b[39m, in \u001b[36mFakeImplHolder.register\u001b[39m\u001b[34m(self, func, source, lib, allow_override)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     46\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.qualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33malready has an fake impl registered at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.kernel.source\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dispatch_has_kernel_for_dispatch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqualname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     52\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.qualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mregister_fake.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: operator torchvision::nms does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2044\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2043\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2044\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2045\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2238\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2236\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2235\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:42\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[INFO] Running in WANDB offline mode\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TrainingArguments\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2132\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2131\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2132\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m   2133\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Are this object\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms requirements defined correctly?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2134\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2136\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: Could not import module 'PreTrainedModel'. Are this object's requirements defined correctly?",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2044\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2043\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2044\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2045\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2238\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2236\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2235\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\trainer.py:39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# ruff: isort: off\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     40\u001b[39m     get_reporting_integration_callbacks,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# ruff: isort: on\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2132\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2131\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2132\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m   2133\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Are this object\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms requirements defined correctly?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2134\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2136\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: Could not import module 'get_reporting_integration_callbacks'. Are this object's requirements defined correctly?",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[32m      3\u001b[39m trainer = Trainer(\n\u001b[32m      4\u001b[39m     model=model,\n\u001b[32m      5\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     processing_class=tokenizer\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2132\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2128\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m   2129\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Are this object\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms requirements defined correctly?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2130\u001b[39m                     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2131\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2132\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m   2133\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Are this object\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms requirements defined correctly?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2134\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2136\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m   2137\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: Could not import module 'Trainer'. Are this object's requirements defined correctly?"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7511c3",
   "metadata": {},
   "source": [
    "Now we can just start the `Trainer` and wait for training to finish. Depending on whether you run it on the full or a subset of the training set this will take 20 or 2 hours, respectively, so grab a few coffees and a good book to read!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503cc912",
   "metadata": {},
   "source": [
    "After training completes, we can push the model and tokenizer to the Hub:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19512c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0e3c8",
   "metadata": {},
   "source": [
    "## 4. Code generation with a pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe79ee",
   "metadata": {},
   "source": [
    "Now is the moment of truth: letâ€™s see how well the trained model actually works! We can see in the logs that the loss went down steadily, but to put the model to the test letâ€™s take a look at how well it works on some prompts. To do that weâ€™ll wrap the model in a text generation `pipeline`, and weâ€™ll put it on the GPU for fast generations if there is one available:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "generator = pipeline(\"text-generation\", \"arraypowerplay/codeparrot-ds-pretrain-gpt2\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a39a1",
   "metadata": {},
   "source": [
    "Letâ€™s start with the simple task of creating a scatter plot:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "print(generator(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c7eeb0",
   "metadata": {},
   "source": [
    "The result looks correct. Does it also work for a pandas operation? Letâ€™s see if we can create a DataFrame from two arrays:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02125b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create dataframe from x and y\n",
    "\"\"\"\n",
    "print(generator(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3eab2",
   "metadata": {},
   "source": [
    "Nice, thatâ€™s the correct answer â€” although it then inserts the column x again. Since the number of generated tokens is limited, the following `for` loop is cut off. Letâ€™s see if we can do something a bit more complex and have the model help us use the `groupby` operation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985da9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# dataframe with profession, income and name\n",
    "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
    "\n",
    "# calculate the mean income per profession\n",
    "\"\"\"\n",
    "print(generator(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc528a4b",
   "metadata": {},
   "source": [
    "Finally, letâ€™s see if we can also use it for `scikit-learn` and set up a Random Forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe5e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\n",
    "# import random forest regressor from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# fit random forest model with 300 estimators on X, y:\n",
    "\"\"\"\n",
    "print(generator(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fb959",
   "metadata": {},
   "source": [
    "Looking at these few examples, it seems that the model has learned some of the syntax of the Python data science stack (of course, we would need to evaluate it more thoroughly before deploying the model in the real world). Sometimes it requires more customization of the model training to achieve the necessary performance for a given use case, however. For example, what if we would like to dynamically update the batch size or have a conditional training loop that skips bad examples on the fly? One option would be to subclass the `Trainer` and add the necessary changes, but sometimes itâ€™s simpler to write the training loop from scratch. Thatâ€™s where ðŸ¤— Accelerate comes in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d0488",
   "metadata": {},
   "source": [
    "## 5. Training with ðŸ¤— Accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d16150",
   "metadata": {},
   "source": [
    "Weâ€™ve seen how to train a model with the `Trainer`, which can allow for some customization. However, sometimes we want full control over the training loop, or we want to make some exotic changes. In this case ðŸ¤— Accelerate is a great choice, and in this section weâ€™ll go through the steps to use it to train our model. To make things more interesting, weâ€™ll also add a twist to the training loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ec5d1",
   "metadata": {},
   "source": [
    "Since we are mainly interested in sensible autocompletion for the data science libraries, it makes sense to give more weight to training samples that make more use of these libraries. We can easily identify these examples through the use of keywords such as `plt`, `pd`, `sk`, `fit`, and `predict`, which are the most frequent import names for `matplotlib.pyplot`, `pandas`, and `sklearn` as well as the fit/predict pattern of the latter. If these are each represented as a single token, we can easily check if they occur in the input sequence. Tokens can have a whitespace prefix, so weâ€™ll also check for those versions in the tokenizer vocabulary. To verify that it works, weâ€™ll add one test token which should be split into multiple tokens:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0aeef87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword has not single token: testthumb\n"
     ]
    }
   ],
   "source": [
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"plt\",\n",
    "    'pd',\n",
    "    'sk',\n",
    "    'fit',\n",
    "    'predict',\n",
    "    ' plt',\n",
    "    ' pd',\n",
    "    ' sk',\n",
    "    ' fit',\n",
    "    ' predict',\n",
    "    'testthumb'\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97575348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8436, 4289, 1201, 2770, 5431, 2564, 2604, 2110, 2872, 4969]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keytoken_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7845043",
   "metadata": {},
   "source": [
    "Great, that seems to work nicely! We can now write a custom loss function that takes the input sequence, the logits, and the key tokens we just selected as inputs. First we need to align the logits and inputs: the input sequence shifted by one to the right forms the labels, since the next token is the label for the current token. We can achieve this by starting the labels from the second token of the input sequence, since the model does not make a prediction for the first token anyway. Then we cut off the last logit, as we donâ€™t have a label for the token that follows the full input sequence. With that we can compute the loss per sample and count the occurrences of all keywords in each sample. Finally, we calculate the weighted average over all samples using the occurrences as weights. Since we donâ€™t want to throw away all the samples that have no keywords, we add 1 to the weights:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cf9bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def keytoken_weighted_loss(labels, logits, keytoken_ids, alpha=1.0):\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1].contigous()\n",
    "    loss_ce = CrossEntropyLoss(reduction='none')\n",
    "    loss = loss_ce(\n",
    "        shift_logits.reshape(-1, shift_logits.shape[-1]), \n",
    "        shift_labels.reshape(-1)\n",
    "    )\n",
    "    loss_per_sample = loss.reshape(shift_logits.shape[0], shift_logits.shape[1]).mean(axis=-1)\n",
    "    weights = torch.stack([(labels == kw).float() for kw in keytoken_ids]).sum(\n",
    "        dim=[0, 2]\n",
    "    )\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed78ee",
   "metadata": {},
   "source": [
    "Before we can start training with this awesome new loss function, we need to prepare a few things:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e54f70",
   "metadata": {},
   "source": [
    "* We need dataloaders to load the data in batches.\n",
    "\n",
    "* We need to set up weight decay parameters.\n",
    "\n",
    "* From time to time we want to evaluate, so it makes sense to wrap the evaluation code in a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb8701",
   "metadata": {},
   "source": [
    "Letâ€™s start with the dataloaders. We only need to set the datasetâ€™s format to `\"torch\"`, and then we can pass it to a PyTorch `DataLoader` with the appropriate batch size:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b32b361",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtokenized_datasets\u001b[49m.set_format(\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m train_dataloader = DataLoader(\n\u001b[32m      5\u001b[39m     tokenized_datasets[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m     batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m      7\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m eval_dataloader = DataLoader(\n\u001b[32m     10\u001b[39m     tokenized_datasets[\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     11\u001b[39m     batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m     12\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7841a",
   "metadata": {},
   "source": [
    "Next, we group the parameters so that the optimizer knows which ones will get an additional weight decay. Usually, all bias and LayerNorm weights terms are exempt from this; hereâ€™s how we can do this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4403da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"param\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"param\": params_without_wd, \"weight_decay\": 0.0}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745db33",
   "metadata": {},
   "source": [
    "Since we want to evaluate the model regularly on the validation set during training, letâ€™s write a function for that as well. It just runs through the evaluation dataloader and gathers all the losses across processes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee299d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "        losses.append(accelerator.gather(outputs.loss))\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea8910",
   "metadata": {},
   "source": [
    "With the `evaluate()` function we can report loss and perplexity at regular intervals. Next, we redefine our model to make sure we train from scratch again:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05b793c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2LMHeadModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mGPT2LMHeadModel\u001b[49m(config)\n",
      "\u001b[31mNameError\u001b[39m: name 'GPT2LMHeadModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c306ecdd",
   "metadata": {},
   "source": [
    "We can then define our optimizer, using the function from before to split the parameters for weight decay:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e08ef",
   "metadata": {},
   "source": [
    "Now letâ€™s prepare the model, optimizer, and dataloaders so we can start training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb5bf0",
   "metadata": {},
   "source": [
    "Now that we have sent our `train_dataloader` to `accelerator.prepare()`, we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_training_steps = num_train_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce57cad",
   "metadata": {},
   "source": [
    "Lastly, we need to set up a bit to push our model to the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import get_full_repo_name, HfApi, create_repo\n",
    "\n",
    "model_name = \"codeparrot-ds-pretrain-gpt2-accelerate\"\n",
    "output_dir = \"codeparrot-ds-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "create_repo(repo_name, exist_ok=True)\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2e951",
   "metadata": {},
   "source": [
    "Before we train, letâ€™s run a quick test to see if the evaluation function works properly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922cb9c2",
   "metadata": {},
   "source": [
    "Those are very high values for loss and perplexity, but thatâ€™s not surprising as we havenâ€™t trained the model yet. With that, we have everything prepared to write the core part of the training script: the training loop. In the training loop we iterate over the dataloader and pass the batches to the model. With the logits, we can then evaluate our custom loss function. We scale the loss by the number of gradient accumulation steps so as not to create larger losses when aggregating more steps. Before we optimize, we also clip the gradients for better convergence. Finally, every few steps we evaluate the model on the evaluation set with our new `evaluate()` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b483a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "eval_steps = 5_000\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
    "    ):\n",
    "        outputs = model(batch[\"input_ids\"])\n",
    "        logits = outputs.logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "\n",
    "        # Print training loss after each 1_000 batch\n",
    "        if step % 1_000 == 0:\n",
    "            accelerator.print({\n",
    "                \"steps\": completed_steps,\n",
    "                \"loss/train\": loss.item()\n",
    "            })\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        scaled_loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(scaled_loss)\n",
    "\n",
    "        # Update parameters\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.paramaters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        \n",
    "        # Evaluation\n",
    "        if step % (gradient_accumulation_steps * eval_steps) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\n",
    "                \"loss/eval\": eval_loss,\n",
    "                \"perplexity\": perplexity\n",
    "            })\n",
    "            model.train()\n",
    "\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                api.upload_folder(\n",
    "                    repo_id=repo_name,\n",
    "                    folder_path=output_dir,\n",
    "                    commit_message=f\"Training progress... in step {step}.\"\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
