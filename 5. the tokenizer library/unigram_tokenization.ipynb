{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15b97e4",
   "metadata": {},
   "source": [
    "# Unigram Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f74480",
   "metadata": {},
   "source": [
    "## 1. Implementing Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ffa91",
   "metadata": {},
   "source": [
    "Now let‚Äôs implement everything we‚Äôve seen so far in code. Like with BPE and WordPiece, this is not an efficient implementation of the Unigram algorithm (quite the opposite), but it should help you understand it a bit better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395cec4a",
   "metadata": {},
   "source": [
    "We will use the same corpus as before as an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2640bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc8664",
   "metadata": {},
   "source": [
    "This time, we will use `xlnet-base-cased` as our model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37200a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0b1f7",
   "metadata": {},
   "source": [
    "Like for BPE and WordPiece, we begin by counting the number of occurrences of each word in the corpus:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f5836e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'‚ñÅThis': 3,\n",
       "             '‚ñÅis': 2,\n",
       "             '‚ñÅthe': 1,\n",
       "             '‚ñÅHugging': 1,\n",
       "             '‚ñÅFace': 1,\n",
       "             '‚ñÅCourse.': 1,\n",
       "             '‚ñÅchapter': 1,\n",
       "             '‚ñÅabout': 1,\n",
       "             '‚ñÅtokenization.': 1,\n",
       "             '‚ñÅsection': 1,\n",
       "             '‚ñÅshows': 1,\n",
       "             '‚ñÅseveral': 1,\n",
       "             '‚ñÅtokenizer': 1,\n",
       "             '‚ñÅalgorithms.': 1,\n",
       "             '‚ñÅHopefully,': 1,\n",
       "             '‚ñÅyou': 1,\n",
       "             '‚ñÅwill': 1,\n",
       "             '‚ñÅbe': 1,\n",
       "             '‚ñÅable': 1,\n",
       "             '‚ñÅto': 1,\n",
       "             '‚ñÅunderstand': 1,\n",
       "             '‚ñÅhow': 1,\n",
       "             '‚ñÅthey': 1,\n",
       "             '‚ñÅare': 1,\n",
       "             '‚ñÅtrained': 1,\n",
       "             '‚ñÅand': 1,\n",
       "             '‚ñÅgenerate': 1,\n",
       "             '‚ñÅtokens.': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    pre_tokenized_result = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenized_result]\n",
    "    for word in pre_tokenized_text:\n",
    "        word_freqs[word] += 1\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37e940",
   "metadata": {},
   "source": [
    "Then, we need to initialize our vocabulary to something larger than the vocab size we will want at the end. We have to include all the basic characters (otherwise we won‚Äôt be able to tokenize every word), but for the bigger substrings we‚Äôll only keep the most common ones, so we sort them by frequency:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72186973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ñÅt', 7),\n",
       " ('is', 5),\n",
       " ('er', 5),\n",
       " ('‚ñÅa', 5),\n",
       " ('‚ñÅto', 4),\n",
       " ('to', 4),\n",
       " ('en', 4),\n",
       " ('‚ñÅT', 3),\n",
       " ('‚ñÅTh', 3),\n",
       " ('‚ñÅThi', 3),\n",
       " ('‚ñÅThis', 3),\n",
       " ('Th', 3),\n",
       " ('Thi', 3),\n",
       " ('This', 3),\n",
       " ('hi', 3),\n",
       " ('his', 3),\n",
       " ('th', 3),\n",
       " ('ou', 3),\n",
       " ('se', 3),\n",
       " ('‚ñÅtok', 3),\n",
       " ('‚ñÅtoke', 3),\n",
       " ('‚ñÅtoken', 3),\n",
       " ('tok', 3),\n",
       " ('toke', 3),\n",
       " ('token', 3),\n",
       " ('ok', 3),\n",
       " ('oke', 3),\n",
       " ('oken', 3),\n",
       " ('ke', 3),\n",
       " ('ken', 3),\n",
       " ('‚ñÅs', 3),\n",
       " ('ra', 3),\n",
       " ('nd', 3),\n",
       " ('‚ñÅi', 2),\n",
       " ('‚ñÅis', 2),\n",
       " ('‚ñÅth', 2),\n",
       " ('‚ñÅthe', 2),\n",
       " ('the', 2),\n",
       " ('he', 2),\n",
       " ('‚ñÅH', 2),\n",
       " ('in', 2),\n",
       " ('rs', 2),\n",
       " ('te', 2),\n",
       " ('‚ñÅab', 2),\n",
       " ('ab', 2),\n",
       " ('‚ñÅtokeni', 2),\n",
       " ('‚ñÅtokeniz', 2),\n",
       " ('tokeni', 2),\n",
       " ('tokeniz', 2),\n",
       " ('okeni', 2),\n",
       " ('okeniz', 2),\n",
       " ('keni', 2),\n",
       " ('keniz', 2),\n",
       " ('eni', 2),\n",
       " ('eniz', 2),\n",
       " ('ni', 2),\n",
       " ('niz', 2),\n",
       " ('iz', 2),\n",
       " ('at', 2),\n",
       " ('ti', 2),\n",
       " ('tio', 2),\n",
       " ('tion', 2),\n",
       " ('io', 2),\n",
       " ('ion', 2),\n",
       " ('on', 2),\n",
       " ('‚ñÅse', 2),\n",
       " ('ho', 2),\n",
       " ('how', 2),\n",
       " ('ow', 2),\n",
       " ('era', 2),\n",
       " ('al', 2),\n",
       " ('s.', 2),\n",
       " ('ll', 2),\n",
       " ('an', 2),\n",
       " ('and', 2),\n",
       " ('ne', 2),\n",
       " ('‚ñÅHu', 1),\n",
       " ('‚ñÅHug', 1),\n",
       " ('‚ñÅHugg', 1),\n",
       " ('‚ñÅHuggi', 1),\n",
       " ('‚ñÅHuggin', 1),\n",
       " ('‚ñÅHugging', 1),\n",
       " ('Hu', 1),\n",
       " ('Hug', 1),\n",
       " ('Hugg', 1),\n",
       " ('Huggi', 1),\n",
       " ('Huggin', 1),\n",
       " ('Hugging', 1),\n",
       " ('ug', 1),\n",
       " ('ugg', 1),\n",
       " ('uggi', 1),\n",
       " ('uggin', 1),\n",
       " ('ugging', 1),\n",
       " ('gg', 1),\n",
       " ('ggi', 1),\n",
       " ('ggin', 1),\n",
       " ('gging', 1),\n",
       " ('gi', 1),\n",
       " ('gin', 1),\n",
       " ('ging', 1),\n",
       " ('ing', 1),\n",
       " ('ng', 1),\n",
       " ('‚ñÅF', 1),\n",
       " ('‚ñÅFa', 1),\n",
       " ('‚ñÅFac', 1),\n",
       " ('‚ñÅFace', 1),\n",
       " ('Fa', 1),\n",
       " ('Fac', 1),\n",
       " ('Face', 1),\n",
       " ('ac', 1),\n",
       " ('ace', 1),\n",
       " ('ce', 1),\n",
       " ('‚ñÅC', 1),\n",
       " ('‚ñÅCo', 1),\n",
       " ('‚ñÅCou', 1),\n",
       " ('‚ñÅCour', 1),\n",
       " ('‚ñÅCours', 1),\n",
       " ('‚ñÅCourse', 1),\n",
       " ('‚ñÅCourse.', 1),\n",
       " ('Co', 1),\n",
       " ('Cou', 1),\n",
       " ('Cour', 1),\n",
       " ('Cours', 1),\n",
       " ('Course', 1),\n",
       " ('Course.', 1),\n",
       " ('our', 1),\n",
       " ('ours', 1),\n",
       " ('ourse', 1),\n",
       " ('ourse.', 1),\n",
       " ('ur', 1),\n",
       " ('urs', 1),\n",
       " ('urse', 1),\n",
       " ('urse.', 1),\n",
       " ('rse', 1),\n",
       " ('rse.', 1),\n",
       " ('se.', 1),\n",
       " ('e.', 1),\n",
       " ('‚ñÅc', 1),\n",
       " ('‚ñÅch', 1),\n",
       " ('‚ñÅcha', 1),\n",
       " ('‚ñÅchap', 1),\n",
       " ('‚ñÅchapt', 1),\n",
       " ('‚ñÅchapte', 1),\n",
       " ('‚ñÅchapter', 1),\n",
       " ('ch', 1),\n",
       " ('cha', 1),\n",
       " ('chap', 1),\n",
       " ('chapt', 1),\n",
       " ('chapte', 1),\n",
       " ('chapter', 1),\n",
       " ('ha', 1),\n",
       " ('hap', 1),\n",
       " ('hapt', 1),\n",
       " ('hapte', 1),\n",
       " ('hapter', 1),\n",
       " ('ap', 1),\n",
       " ('apt', 1),\n",
       " ('apte', 1),\n",
       " ('apter', 1),\n",
       " ('pt', 1),\n",
       " ('pte', 1),\n",
       " ('pter', 1),\n",
       " ('ter', 1),\n",
       " ('‚ñÅabo', 1),\n",
       " ('‚ñÅabou', 1),\n",
       " ('‚ñÅabout', 1),\n",
       " ('abo', 1),\n",
       " ('abou', 1),\n",
       " ('about', 1),\n",
       " ('bo', 1),\n",
       " ('bou', 1),\n",
       " ('bout', 1),\n",
       " ('out', 1),\n",
       " ('ut', 1),\n",
       " ('‚ñÅtokeniza', 1),\n",
       " ('‚ñÅtokenizat', 1),\n",
       " ('‚ñÅtokenizati', 1),\n",
       " ('‚ñÅtokenizatio', 1),\n",
       " ('‚ñÅtokenization', 1),\n",
       " ('‚ñÅtokenization.', 1),\n",
       " ('tokeniza', 1),\n",
       " ('tokenizat', 1),\n",
       " ('tokenizati', 1),\n",
       " ('tokenizatio', 1),\n",
       " ('tokenization', 1),\n",
       " ('tokenization.', 1),\n",
       " ('okeniza', 1),\n",
       " ('okenizat', 1),\n",
       " ('okenizati', 1),\n",
       " ('okenizatio', 1),\n",
       " ('okenization', 1),\n",
       " ('okenization.', 1),\n",
       " ('keniza', 1),\n",
       " ('kenizat', 1),\n",
       " ('kenizati', 1),\n",
       " ('kenizatio', 1),\n",
       " ('kenization', 1),\n",
       " ('kenization.', 1),\n",
       " ('eniza', 1),\n",
       " ('enizat', 1),\n",
       " ('enizati', 1),\n",
       " ('enizatio', 1),\n",
       " ('enization', 1),\n",
       " ('enization.', 1),\n",
       " ('niza', 1),\n",
       " ('nizat', 1),\n",
       " ('nizati', 1),\n",
       " ('nizatio', 1),\n",
       " ('nization', 1),\n",
       " ('nization.', 1),\n",
       " ('iza', 1),\n",
       " ('izat', 1),\n",
       " ('izati', 1),\n",
       " ('izatio', 1),\n",
       " ('ization', 1),\n",
       " ('ization.', 1),\n",
       " ('za', 1),\n",
       " ('zat', 1),\n",
       " ('zati', 1),\n",
       " ('zatio', 1),\n",
       " ('zation', 1),\n",
       " ('zation.', 1),\n",
       " ('ati', 1),\n",
       " ('atio', 1),\n",
       " ('ation', 1),\n",
       " ('ation.', 1),\n",
       " ('tion.', 1),\n",
       " ('ion.', 1),\n",
       " ('on.', 1),\n",
       " ('n.', 1),\n",
       " ('‚ñÅsec', 1),\n",
       " ('‚ñÅsect', 1),\n",
       " ('‚ñÅsecti', 1),\n",
       " ('‚ñÅsectio', 1),\n",
       " ('‚ñÅsection', 1),\n",
       " ('sec', 1),\n",
       " ('sect', 1),\n",
       " ('secti', 1),\n",
       " ('sectio', 1),\n",
       " ('section', 1),\n",
       " ('ec', 1),\n",
       " ('ect', 1),\n",
       " ('ecti', 1),\n",
       " ('ectio', 1),\n",
       " ('ection', 1),\n",
       " ('ct', 1),\n",
       " ('cti', 1),\n",
       " ('ctio', 1),\n",
       " ('ction', 1),\n",
       " ('‚ñÅsh', 1),\n",
       " ('‚ñÅsho', 1),\n",
       " ('‚ñÅshow', 1),\n",
       " ('‚ñÅshows', 1),\n",
       " ('sh', 1),\n",
       " ('sho', 1),\n",
       " ('show', 1),\n",
       " ('shows', 1),\n",
       " ('hows', 1),\n",
       " ('ows', 1),\n",
       " ('ws', 1),\n",
       " ('‚ñÅsev', 1),\n",
       " ('‚ñÅseve', 1),\n",
       " ('‚ñÅsever', 1),\n",
       " ('‚ñÅsevera', 1),\n",
       " ('‚ñÅseveral', 1),\n",
       " ('sev', 1),\n",
       " ('seve', 1),\n",
       " ('sever', 1),\n",
       " ('severa', 1),\n",
       " ('several', 1),\n",
       " ('ev', 1),\n",
       " ('eve', 1),\n",
       " ('ever', 1),\n",
       " ('evera', 1),\n",
       " ('everal', 1),\n",
       " ('ve', 1),\n",
       " ('ver', 1),\n",
       " ('vera', 1),\n",
       " ('veral', 1),\n",
       " ('eral', 1),\n",
       " ('ral', 1),\n",
       " ('‚ñÅtokenize', 1),\n",
       " ('‚ñÅtokenizer', 1),\n",
       " ('tokenize', 1),\n",
       " ('tokenizer', 1),\n",
       " ('okenize', 1),\n",
       " ('okenizer', 1),\n",
       " ('kenize', 1),\n",
       " ('kenizer', 1),\n",
       " ('enize', 1),\n",
       " ('enizer', 1),\n",
       " ('nize', 1),\n",
       " ('nizer', 1),\n",
       " ('ize', 1),\n",
       " ('izer', 1),\n",
       " ('ze', 1),\n",
       " ('zer', 1),\n",
       " ('‚ñÅal', 1),\n",
       " ('‚ñÅalg', 1),\n",
       " ('‚ñÅalgo', 1),\n",
       " ('‚ñÅalgor', 1),\n",
       " ('‚ñÅalgori', 1),\n",
       " ('‚ñÅalgorit', 1),\n",
       " ('‚ñÅalgorith', 1),\n",
       " ('‚ñÅalgorithm', 1),\n",
       " ('‚ñÅalgorithms', 1),\n",
       " ('‚ñÅalgorithms.', 1),\n",
       " ('alg', 1),\n",
       " ('algo', 1),\n",
       " ('algor', 1),\n",
       " ('algori', 1),\n",
       " ('algorit', 1),\n",
       " ('algorith', 1),\n",
       " ('algorithm', 1),\n",
       " ('algorithms', 1),\n",
       " ('algorithms.', 1),\n",
       " ('lg', 1),\n",
       " ('lgo', 1),\n",
       " ('lgor', 1),\n",
       " ('lgori', 1),\n",
       " ('lgorit', 1),\n",
       " ('lgorith', 1),\n",
       " ('lgorithm', 1),\n",
       " ('lgorithms', 1),\n",
       " ('lgorithms.', 1),\n",
       " ('go', 1),\n",
       " ('gor', 1),\n",
       " ('gori', 1),\n",
       " ('gorit', 1),\n",
       " ('gorith', 1),\n",
       " ('gorithm', 1),\n",
       " ('gorithms', 1),\n",
       " ('gorithms.', 1),\n",
       " ('or', 1),\n",
       " ('ori', 1),\n",
       " ('orit', 1),\n",
       " ('orith', 1),\n",
       " ('orithm', 1),\n",
       " ('orithms', 1),\n",
       " ('orithms.', 1),\n",
       " ('ri', 1),\n",
       " ('rit', 1),\n",
       " ('rith', 1),\n",
       " ('rithm', 1),\n",
       " ('rithms', 1),\n",
       " ('rithms.', 1),\n",
       " ('it', 1),\n",
       " ('ith', 1),\n",
       " ('ithm', 1),\n",
       " ('ithms', 1),\n",
       " ('ithms.', 1),\n",
       " ('thm', 1),\n",
       " ('thms', 1),\n",
       " ('thms.', 1),\n",
       " ('hm', 1),\n",
       " ('hms', 1),\n",
       " ('hms.', 1),\n",
       " ('ms', 1),\n",
       " ('ms.', 1),\n",
       " ('‚ñÅHo', 1),\n",
       " ('‚ñÅHop', 1),\n",
       " ('‚ñÅHope', 1),\n",
       " ('‚ñÅHopef', 1),\n",
       " ('‚ñÅHopefu', 1),\n",
       " ('‚ñÅHopeful', 1),\n",
       " ('‚ñÅHopefull', 1),\n",
       " ('‚ñÅHopefully', 1),\n",
       " ('‚ñÅHopefully,', 1),\n",
       " ('Ho', 1),\n",
       " ('Hop', 1),\n",
       " ('Hope', 1),\n",
       " ('Hopef', 1),\n",
       " ('Hopefu', 1),\n",
       " ('Hopeful', 1),\n",
       " ('Hopefull', 1),\n",
       " ('Hopefully', 1),\n",
       " ('Hopefully,', 1),\n",
       " ('op', 1),\n",
       " ('ope', 1),\n",
       " ('opef', 1),\n",
       " ('opefu', 1),\n",
       " ('opeful', 1),\n",
       " ('opefull', 1),\n",
       " ('opefully', 1),\n",
       " ('opefully,', 1),\n",
       " ('pe', 1),\n",
       " ('pef', 1),\n",
       " ('pefu', 1),\n",
       " ('peful', 1),\n",
       " ('pefull', 1),\n",
       " ('pefully', 1),\n",
       " ('pefully,', 1),\n",
       " ('ef', 1),\n",
       " ('efu', 1),\n",
       " ('eful', 1),\n",
       " ('efull', 1),\n",
       " ('efully', 1),\n",
       " ('efully,', 1),\n",
       " ('fu', 1),\n",
       " ('ful', 1),\n",
       " ('full', 1),\n",
       " ('fully', 1),\n",
       " ('fully,', 1),\n",
       " ('ul', 1),\n",
       " ('ull', 1),\n",
       " ('ully', 1),\n",
       " ('ully,', 1),\n",
       " ('lly', 1),\n",
       " ('lly,', 1),\n",
       " ('ly', 1),\n",
       " ('ly,', 1),\n",
       " ('y,', 1),\n",
       " ('‚ñÅy', 1),\n",
       " ('‚ñÅyo', 1),\n",
       " ('‚ñÅyou', 1),\n",
       " ('yo', 1),\n",
       " ('you', 1),\n",
       " ('‚ñÅw', 1),\n",
       " ('‚ñÅwi', 1),\n",
       " ('‚ñÅwil', 1),\n",
       " ('‚ñÅwill', 1),\n",
       " ('wi', 1),\n",
       " ('wil', 1),\n",
       " ('will', 1),\n",
       " ('il', 1),\n",
       " ('ill', 1),\n",
       " ('‚ñÅb', 1),\n",
       " ('‚ñÅbe', 1),\n",
       " ('be', 1),\n",
       " ('‚ñÅabl', 1),\n",
       " ('‚ñÅable', 1),\n",
       " ('abl', 1),\n",
       " ('able', 1),\n",
       " ('bl', 1),\n",
       " ('ble', 1),\n",
       " ('le', 1),\n",
       " ('‚ñÅu', 1),\n",
       " ('‚ñÅun', 1),\n",
       " ('‚ñÅund', 1),\n",
       " ('‚ñÅunde', 1),\n",
       " ('‚ñÅunder', 1),\n",
       " ('‚ñÅunders', 1),\n",
       " ('‚ñÅunderst', 1),\n",
       " ('‚ñÅundersta', 1),\n",
       " ('‚ñÅunderstan', 1),\n",
       " ('‚ñÅunderstand', 1),\n",
       " ('un', 1),\n",
       " ('und', 1),\n",
       " ('unde', 1),\n",
       " ('under', 1),\n",
       " ('unders', 1),\n",
       " ('underst', 1),\n",
       " ('understa', 1),\n",
       " ('understan', 1),\n",
       " ('understand', 1),\n",
       " ('nde', 1),\n",
       " ('nder', 1),\n",
       " ('nders', 1),\n",
       " ('nderst', 1),\n",
       " ('ndersta', 1),\n",
       " ('nderstan', 1),\n",
       " ('nderstand', 1),\n",
       " ('de', 1),\n",
       " ('der', 1),\n",
       " ('ders', 1),\n",
       " ('derst', 1),\n",
       " ('dersta', 1),\n",
       " ('derstan', 1),\n",
       " ('derstand', 1),\n",
       " ('ers', 1),\n",
       " ('erst', 1),\n",
       " ('ersta', 1),\n",
       " ('erstan', 1),\n",
       " ('erstand', 1),\n",
       " ('rst', 1),\n",
       " ('rsta', 1),\n",
       " ('rstan', 1),\n",
       " ('rstand', 1),\n",
       " ('st', 1),\n",
       " ('sta', 1),\n",
       " ('stan', 1),\n",
       " ('stand', 1),\n",
       " ('ta', 1),\n",
       " ('tan', 1),\n",
       " ('tand', 1),\n",
       " ('‚ñÅh', 1),\n",
       " ('‚ñÅho', 1),\n",
       " ('‚ñÅhow', 1),\n",
       " ('‚ñÅthey', 1),\n",
       " ('they', 1),\n",
       " ('hey', 1),\n",
       " ('ey', 1),\n",
       " ('‚ñÅar', 1),\n",
       " ('‚ñÅare', 1),\n",
       " ('ar', 1),\n",
       " ('are', 1),\n",
       " ('re', 1),\n",
       " ('‚ñÅtr', 1),\n",
       " ('‚ñÅtra', 1),\n",
       " ('‚ñÅtrai', 1),\n",
       " ('‚ñÅtrain', 1),\n",
       " ('‚ñÅtraine', 1),\n",
       " ('‚ñÅtrained', 1),\n",
       " ('tr', 1),\n",
       " ('tra', 1),\n",
       " ('trai', 1),\n",
       " ('train', 1),\n",
       " ('traine', 1),\n",
       " ('trained', 1),\n",
       " ('rai', 1),\n",
       " ('rain', 1),\n",
       " ('raine', 1),\n",
       " ('rained', 1),\n",
       " ('ai', 1),\n",
       " ('ain', 1),\n",
       " ('aine', 1),\n",
       " ('ained', 1),\n",
       " ('ine', 1),\n",
       " ('ined', 1),\n",
       " ('ned', 1),\n",
       " ('ed', 1),\n",
       " ('‚ñÅan', 1),\n",
       " ('‚ñÅand', 1),\n",
       " ('‚ñÅg', 1),\n",
       " ('‚ñÅge', 1),\n",
       " ('‚ñÅgen', 1),\n",
       " ('‚ñÅgene', 1),\n",
       " ('‚ñÅgener', 1),\n",
       " ('‚ñÅgenera', 1),\n",
       " ('‚ñÅgenerat', 1),\n",
       " ('‚ñÅgenerate', 1),\n",
       " ('ge', 1),\n",
       " ('gen', 1),\n",
       " ('gene', 1),\n",
       " ('gener', 1),\n",
       " ('genera', 1),\n",
       " ('generat', 1),\n",
       " ('generate', 1),\n",
       " ('ene', 1),\n",
       " ('ener', 1),\n",
       " ('enera', 1),\n",
       " ('enerat', 1),\n",
       " ('enerate', 1),\n",
       " ('ner', 1),\n",
       " ('nera', 1),\n",
       " ('nerat', 1),\n",
       " ('nerate', 1),\n",
       " ('erat', 1),\n",
       " ('erate', 1),\n",
       " ('rat', 1),\n",
       " ('rate', 1),\n",
       " ('ate', 1),\n",
       " ('‚ñÅtokens', 1),\n",
       " ('‚ñÅtokens.', 1),\n",
       " ('tokens', 1),\n",
       " ('tokens.', 1),\n",
       " ('okens', 1),\n",
       " ('okens.', 1),\n",
       " ('kens', 1),\n",
       " ('kens.', 1),\n",
       " ('ens', 1),\n",
       " ('ens.', 1),\n",
       " ('ns', 1),\n",
       " ('ns.', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_freqs = defaultdict(int)\n",
    "subword_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subword_freqs[word[i: j]] += freq\n",
    "\n",
    "# Sort subword by frequency\n",
    "sorted_subwords = sorted(subword_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940a8362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_subwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f42270",
   "metadata": {},
   "source": [
    "We group the characters with the best subwords to arrive at an initial vocabulary of size 300:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c31b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token, freq in token_freqs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93614991",
   "metadata": {},
   "source": [
    "üí° SentencePiece uses a more efficient algorithm called Enhanced Suffix Array (ESA) to create the initial vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618cf11",
   "metadata": {},
   "source": [
    "Next, we compute the sum of all frequencies, to convert the frequencies into probabilities. For our model we will store the logarithms of the probabilities, because it‚Äôs more numerically stable to add logarithms than to multiply small numbers, and this will simplify the computation of the loss of the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f80ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2856a8",
   "metadata": {},
   "source": [
    "Now the main function is the one that tokenizes words using the Viterbi algorithm. As we saw before, that algorithm computes the best segmentation of each substring of the word, which we will store in a variable named `best_segmentations`. We will store one dictionary per position in the word (from 0 to its total length), with two keys: the index of the start of the last token in the best segmentation, and the score of the best segmentation. With the index of the start of the last token, we will be able to retrieve the full segmentation once the list is completely populated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1077a8",
   "metadata": {},
   "source": [
    "Populating the list is done with just two loops: the main loop goes over each start position, and the second loop tries all substrings beginning at that start position. If the substring is in the vocabulary, we have a new segmentation of the word up until that end position, which we compare to what is in `best_segmentations`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8950316",
   "metadata": {},
   "source": [
    "Once the main loop is finished, we just start from the end and hop from one start position to the next, recording the tokens as we go, until we reach the start of the word:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a5df466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    for start_idx in range(len(word)):\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx: end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        return [\"<unk>\"], None\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start: end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start: end])\n",
    "    return tokens, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd6282",
   "metadata": {},
   "source": [
    "We can already try our initial model on some words:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6172c34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hopefully\", model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e6d5897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['b', 'e', 'a', 'ut', 'i', 'f', 'u', 'l'], 39.164374202976724)\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"beautiful\", model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2546590",
   "metadata": {},
   "source": [
    "Now it‚Äôs easy to compute the loss of the model on the corpus!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73ccf423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += word_loss * freq\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdce2b6",
   "metadata": {},
   "source": [
    "We can check it works on the model we have:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cbeb6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413.10377642940875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bc8a4",
   "metadata": {},
   "source": [
    "Computing the scores for each token is not very hard either; we just have to compute the loss for the models obtained by deleting each token:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24fe98e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # We always keep token of length 1\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - compute_loss(model)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f58f3",
   "metadata": {},
   "source": [
    "We can try it on a given token:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59841ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.376412403623874\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "scores = compute_scores(model)\n",
    "print(scores[\"ll\"])\n",
    "print(scores[\"his\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4e90c",
   "metadata": {},
   "source": [
    "With all of this in place, the last thing we need to do is add the special tokens used by the model to the vocabulary, then loop until we have pruned enough tokens from the vocabulary to reach our desired size:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa7f4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c0e74",
   "metadata": {},
   "source": [
    "Then, to tokenize some text, we just need to apply the pre-tokenization and then use our encode_word() function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe8e7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, model):\n",
    "    pre_tokenized_result = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenized_result]\n",
    "    encoded_word = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_word, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a28bbe93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅThis',\n",
       " '‚ñÅis',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅHugging',\n",
       " '‚ñÅFace',\n",
       " '‚ñÅ',\n",
       " 'c',\n",
       " 'ou',\n",
       " 'r',\n",
       " 's',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is the Hugging Face course.\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f9e4f",
   "metadata": {},
   "source": [
    "The XLNetTokenizer uses SentencePiece which is why the `\"_\"` character is included. To decode with SentencePiece, concatenate all the tokens and replace `\"_\"` with a space.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
