{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9ec343",
   "metadata": {},
   "source": [
    "# Training a new tokenizer from an old one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6c800",
   "metadata": {},
   "source": [
    "If a language model is not available in the language you are interested in, or if your corpus is very different from the one your language model was trained on, you will most likely want to retrain the model from scratch using a tokenizer adapted to your data. That will require training a new tokenizer on your dataset. But what exactly does that mean? When we first looked at tokenizers in Chapter 2, we saw that most Transformer models use a `subword tokenization` algorithm. To identify which subwords are of interest and occur most frequently in the corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus â€” a process we call training. The exact rules that govern this training depend on the type of tokenizer used, and weâ€™ll go over the three main algorithms later in this chapter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214ee8c4",
   "metadata": {},
   "source": [
    "## 1. Assembling a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd2952",
   "metadata": {},
   "source": [
    "Thereâ€™s a very simple API in ðŸ¤— Transformers that you can use to train a new tokenizer with the same characteristics as an existing one: `AutoTokenizer.train_new_from_iterator()`. To see this in action, letâ€™s say we want to train GPT-2 from scratch, but in a language other than English. Our first task will be to gather lots of data in that language in a training corpus. To provide examples everyone will be able to understand, we wonâ€™t use a language like Russian or Chinese here, but rather programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59dcfc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92329c490517488bac66b4e558ee7247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pair/train-00000-of-00003.parquet:   0%|          | 0.00/163M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\datasets--sentence-transformers--codesearchnet. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8d807eaebb474f8db0a6715e0a0c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pair/train-00001-of-00003.parquet:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13078b6ca23f4874839b4382de1d12f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pair/train-00002-of-00003.parquet:   0%|          | 0.00/163M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd348751148d440da6deb5d30ee259bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['comment', 'code'],\n",
      "        num_rows: 1375067\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/sentence-transformers/codesearchnet/resolve/main/pair/\"\n",
    "data_files = {\n",
    "    \"train\": [\n",
    "        f\"{base_url}train-00000-of-00003.parquet\",\n",
    "        f\"{base_url}train-00001-of-00003.parquet\",\n",
    "        f\"{base_url}train-00002-of-00003.parquet\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "raw_datasets = load_dataset(\"parquet\", data_files=data_files)\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "474d49ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creates (or updates) a new ProjectStatus for the given build and\\n        returns it.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][15][\"comment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ba0e9",
   "metadata": {},
   "source": [
    "Here we will create a new column named `whole_function` to conclude `comment` and `code` column into one concatenated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c294b117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f9838307214508a8ad0cb514a9f903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1375067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def merge_columns(examples):\n",
    "    comments = examples[\"comment\"]\n",
    "    codes = examples[\"code\"]\n",
    "    whole_function = [\n",
    "        f\"{comment}\\n{code}\" for comment, code in zip(comments, codes) \n",
    "    ]\n",
    "    return {\"whole_function\": whole_function}\n",
    "\n",
    "raw_datasets = raw_datasets.map(merge_columns, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9166bfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comment': 'Computes the new parent id for the node being moved.\\n\\n@return int', 'code': \"protected function parentId()\\n\\t{\\n\\t\\tswitch ( $this->position )\\n\\t\\t{\\n\\t\\t\\tcase 'root':\\n\\t\\t\\t\\treturn null;\\n\\n\\t\\t\\tcase 'child':\\n\\t\\t\\t\\treturn $this->target->getKey();\\n\\n\\t\\t\\tdefault:\\n\\t\\t\\t\\treturn $this->target->getParentId();\\n\\t\\t}\\n\\t}\", 'whole_function': \"Computes the new parent id for the node being moved.\\n\\n@return int\\nprotected function parentId()\\n\\t{\\n\\t\\tswitch ( $this->position )\\n\\t\\t{\\n\\t\\t\\tcase 'root':\\n\\t\\t\\t\\treturn null;\\n\\n\\t\\t\\tcase 'child':\\n\\t\\t\\t\\treturn $this->target->getKey();\\n\\n\\t\\t\\tdefault:\\n\\t\\t\\t\\treturn $this->target->getParentId();\\n\\t\\t}\\n\\t}\"}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa44b3",
   "metadata": {},
   "source": [
    "The first thing we need to do is transform the dataset into an `iterator` of lists of texts â€” for instance, a list of list of texts. Using lists of texts will enable our tokenizer to go faster (training on batches of texts instead of processing individual texts one by one), and it should be an iterator if we want to avoid having everything in memory at once. If your corpus is huge, you will want to take advantage of the fact that ðŸ¤— Datasets does not load everything into RAM but stores the elements of the dataset on disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8409a6",
   "metadata": {},
   "source": [
    "Using a Python generator, we can avoid Python loading anything into memory until itâ€™s actually necessary. To create such a generator, you just to need to replace the brackets with parentheses:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "876fe15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i: i + 1000][\"whole_function\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e4f5dc",
   "metadata": {},
   "source": [
    "This line of code doesnâ€™t fetch any elements of the dataset; it just creates an object you can use in a Python `for` loop. The texts will only be loaded when you need them (that is, when youâ€™re at the step of the `for` loop that requires them), and only 1,000 texts at a time will be loaded. This way you wonâ€™t exhaust all your memory even if you are processing a huge dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defcd7ae",
   "metadata": {},
   "source": [
    "The problem with a `generator` object is that it can only be used once. So, instead of this giving us the list of the first 10 digits twice:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8e34af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "gen = (i for i in range(10))\n",
    "print(list(gen))\n",
    "print(list(gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac7d4a",
   "metadata": {},
   "source": [
    "Thatâ€™s why we define a function that returns a `generator` instead:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3860815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i: i + 1000][\"whole_function\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df8456",
   "metadata": {},
   "source": [
    "You can also define your generator inside a `for` loop by using the `yield` statement:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12146ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    datasets = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(datasets), 1000):\n",
    "        samples = datasets[start_idx: start_idx + 1000]\n",
    "        yield samples[\"whole_function\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0770a62",
   "metadata": {},
   "source": [
    "which will produce the exact same generator as before, but allows you to use more complex logic than you can in a list comprehension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd995d0",
   "metadata": {},
   "source": [
    "## 2. Training a new tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0631d80e",
   "metadata": {},
   "source": [
    "Now that we have our corpus in the form of an iterator of batches of texts, we are ready to train a new tokenizer. To do this, we first need to load the tokenizer we want to pair with our model (here, GPT-2):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2ba4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1219dec",
   "metadata": {},
   "source": [
    "Even though we are going to train a new tokenizer, itâ€™s a good idea to do this to avoid starting entirely from scratch. This way, we wonâ€™t have to specify anything about the tokenization algorithm or the special tokens we want to use; our new tokenizer will be exactly the same as GPT-2, and the only thing that will change is the vocabulary, which will be determined by the training on our corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab78ba",
   "metadata": {},
   "source": [
    "First letâ€™s have a look at how this tokenizer would treat an example function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd6b81ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ä add',\n",
       " '_',\n",
       " 'n',\n",
       " 'umbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ä b',\n",
       " '):',\n",
       " 'ÄŠ',\n",
       " 'Ä ',\n",
       " 'Ä ',\n",
       " 'Ä ',\n",
       " 'Ä \"\"\"',\n",
       " 'Add',\n",
       " 'Ä the',\n",
       " 'Ä two',\n",
       " 'Ä numbers',\n",
       " 'Ä `',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ä and',\n",
       " 'Ä `',\n",
       " 'b',\n",
       " '`',\n",
       " '.\"',\n",
       " '\"\"',\n",
       " 'ÄŠ',\n",
       " 'Ä ',\n",
       " 'Ä ',\n",
       " 'Ä ',\n",
       " 'Ä return',\n",
       " 'Ä a',\n",
       " 'Ä +',\n",
       " 'Ä b']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b538319",
   "metadata": {},
   "source": [
    "This tokenizer has a few special symbols, like `Ä ` and `ÄŠ`, which denote spaces and newlines, respectively. As we can see, this is not too efficient: the tokenizer returns individual tokens for each space, when it could group together indentation levels (since having sets of four or eight spaces is going to be very common in code). It also split the function name a bit weirdly, not being used to seeing words with the `_` character.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984a5d7",
   "metadata": {},
   "source": [
    "Letâ€™s train a new tokenizer and see if it solves those issues. For this, weâ€™ll use the method `train_new_from_iterator()`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc440a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde5af6",
   "metadata": {},
   "source": [
    "Letâ€™s try our brand new tokenizer on the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "639c6231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ä add',\n",
       " '_',\n",
       " 'numbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ä b',\n",
       " '):',\n",
       " 'ÄŠÄ Ä Ä ',\n",
       " 'Ä \"\"\"',\n",
       " 'Add',\n",
       " 'Ä the',\n",
       " 'Ä two',\n",
       " 'Ä numbers',\n",
       " 'Ä `',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ä and',\n",
       " 'Ä `',\n",
       " 'b',\n",
       " '`.',\n",
       " '\"\"\"',\n",
       " 'ÄŠÄ Ä Ä ',\n",
       " 'Ä return',\n",
       " 'Ä a',\n",
       " 'Ä +',\n",
       " 'Ä b']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7164079e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d36b887",
   "metadata": {},
   "source": [
    "Letâ€™s look at another example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26441dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'Ä Linear',\n",
       " 'Layer',\n",
       " '():',\n",
       " 'ÄŠÄ Ä Ä ',\n",
       " 'Ä def',\n",
       " 'Ä __',\n",
       " 'init',\n",
       " '__(',\n",
       " 'self',\n",
       " ',',\n",
       " 'Ä input',\n",
       " '_',\n",
       " 'size',\n",
       " ',',\n",
       " 'Ä output',\n",
       " '_',\n",
       " 'size',\n",
       " '):',\n",
       " 'ÄŠÄ Ä Ä Ä Ä Ä Ä ',\n",
       " 'Ä self',\n",
       " '.',\n",
       " 'weight',\n",
       " 'Ä =',\n",
       " 'Ä torch',\n",
       " '.',\n",
       " 'rand',\n",
       " 'n',\n",
       " '(',\n",
       " 'input',\n",
       " '_',\n",
       " 'size',\n",
       " ',',\n",
       " 'Ä output',\n",
       " '_',\n",
       " 'size',\n",
       " ')',\n",
       " 'ÄŠÄ Ä Ä Ä Ä Ä Ä ',\n",
       " 'Ä self',\n",
       " '.',\n",
       " 'bias',\n",
       " 'Ä =',\n",
       " 'Ä torch',\n",
       " '.',\n",
       " 'zeros',\n",
       " '(',\n",
       " 'output',\n",
       " '_',\n",
       " 'size',\n",
       " ')',\n",
       " 'ÄŠÄŠÄ Ä Ä ',\n",
       " 'Ä def',\n",
       " 'Ä __',\n",
       " 'call',\n",
       " '__(',\n",
       " 'self',\n",
       " ',',\n",
       " 'Ä x',\n",
       " '):',\n",
       " 'ÄŠÄ Ä Ä Ä Ä Ä Ä ',\n",
       " 'Ä return',\n",
       " 'Ä x',\n",
       " 'Ä @',\n",
       " 'Ä self',\n",
       " '.',\n",
       " 'weights',\n",
       " 'Ä +',\n",
       " 'Ä self',\n",
       " '.',\n",
       " 'bias',\n",
       " 'ÄŠÄ Ä Ä Ä ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "tokenizer.tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e628c343",
   "metadata": {},
   "source": [
    "In addition to the token corresponding to an indentation, here we can also see a token for a double indentation: `ÄŠÄ Ä Ä Ä Ä Ä Ä `. The special Python words like `class`, `init`, `call`, `self`, and `return` are each tokenized as one token, and we can see that as well as splitting on `_` and `.` the tokenizer correctly splits even camel-cased names: `LinearLayer` is tokenized as `[\"Ä Linear\", \"Layer\"]`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c548f8d",
   "metadata": {},
   "source": [
    "## 3. Saving the tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d471f043",
   "metadata": {},
   "source": [
    "To make sure we can use it later, we need to save our new tokenizer. Like for models, this is done with the `save_pretrained()` method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8132953a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer\\\\tokenizer_config.json',\n",
       " 'code-search-net-tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98345458",
   "metadata": {},
   "source": [
    "This will create a new folder named `code-search-net-tokenizer`, which will contain all the files the tokenizer needs to be reloaded. If you want to share this tokenizer with your colleagues and friends, you can upload it to the Hub by logging into your account. If youâ€™re working in a notebook, thereâ€™s a convenience function to help you with this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cae5efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdde767",
   "metadata": {},
   "source": [
    "Once youâ€™ve logged in, you can push your tokenizer by executing the following command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8d1bc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/arraypowerplay/code-search-net-tokenizer/commit/fa5df0981353959138d8da6e774d7300c60c5496', commit_message='Upload tokenizer', commit_description='', oid='fa5df0981353959138d8da6e774d7300c60c5496', pr_url=None, repo_url=RepoUrl('https://huggingface.co/arraypowerplay/code-search-net-tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='arraypowerplay/code-search-net-tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae5051",
   "metadata": {},
   "source": [
    "This will create a new repository in your namespace with the name `code-search-net-tokenizer`, containing the tokenizer file. You can then load the tokenizer from anywhere with the `from_pretrained()` method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d60a048a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879628c804dd4b4a93f05facd686b614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\.conda\\envs\\torch_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--arraypowerplay--code-search-net-tokenizer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a215eb98959c404da7decbaf9fa1e270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"arraypowerplay/code-search-net-tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
