{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf1fd14",
   "metadata": {},
   "source": [
    "# WordPiece Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b1290",
   "metadata": {},
   "source": [
    "## 1. Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211349c",
   "metadata": {},
   "source": [
    "Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17995b9c",
   "metadata": {},
   "source": [
    "<pre>\n",
    "w ##o ##r ##d\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9fbe95",
   "metadata": {},
   "source": [
    "Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5501be",
   "metadata": {},
   "source": [
    "Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0d647",
   "metadata": {},
   "source": [
    "$$score=\\frac{freq\\_of\\_pair}{freq\\_of\\_first\\_element \\times freq\\_of\\_second\\_element}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a7699",
   "metadata": {},
   "source": [
    "By dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary. For instance, it won‚Äôt necessarily merge `(\"un\", \"##able\")` even if that pair occurs very frequently in the vocabulary, because the two pairs `\"un\"` and `\"##able\"` will likely each appear in a lot of other words and have a high frequency. In contrast, a pair like `(\"hu\", \"##gging\")` will probably be merged faster (assuming the word `‚Äúhugging‚Äù` appears often in the vocabulary) since `\"hu\"` and `\"##gging\"` are likely to be less frequent individually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75206c",
   "metadata": {},
   "source": [
    "## 2. Tokenization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21de657",
   "metadata": {},
   "source": [
    "Tokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word `\"hugs\"` the longest subword starting from the beginning that is inside the vocabulary is `\"hug\"`, so we split there and get `[\"hug\", \"##s\"]`. We then continue with `\"##s\"`, which is in the vocabulary, so the tokenization of `\"hugs\"` is `[\"hug\", \"##s\"]`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0143bc2",
   "metadata": {},
   "source": [
    "With BPE, we would have applied the merges learned in order and tokenized this as `[\"hu\", \"##gs\"]`, so the encoding is different.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00cc977",
   "metadata": {},
   "source": [
    "When the tokenization gets to a stage where it‚Äôs not possible to find a subword in the vocabulary, the whole word is tokenized as unknown ‚Äî so, for instance, `\"mug\"` would be tokenized as `[\"[UNK]\"]`, as would `\"bum\"` (even if we can begin with `\"b\"` and `\"##u\"`, `\"##m\"` is not the vocabulary, and the resulting tokenization will just be `[\"[UNK]\"]`, not `[\"b\", \"##u\", \"[UNK]\"]`). This is another difference from BPE, which would only classify the individual characters not in the vocabulary as unknown.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0ea40",
   "metadata": {},
   "source": [
    "## 3. Implementing WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adbe3a",
   "metadata": {},
   "source": [
    "Now let‚Äôs take a look at an implementation of the WordPiece algorithm. Like with BPE, this is just pedagogical, and you won‚Äôt able to use this on a big corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a074ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98381b8d",
   "metadata": {},
   "source": [
    "First, we need to pre-tokenize the corpus into words. Since we are replicating a WordPiece tokenizer (like BERT), we will use the `bert-base-cased` tokenizer for the pre-tokenization:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7d7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecd69d",
   "metadata": {},
   "source": [
    "Then we compute the frequencies of each word in the corpus as we do the pre-tokenization:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1dbc702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'This': 3,\n",
       "             'is': 2,\n",
       "             'the': 1,\n",
       "             'Hugging': 1,\n",
       "             'Face': 1,\n",
       "             'Course': 1,\n",
       "             '.': 4,\n",
       "             'chapter': 1,\n",
       "             'about': 1,\n",
       "             'tokenization': 1,\n",
       "             'section': 1,\n",
       "             'shows': 1,\n",
       "             'several': 1,\n",
       "             'tokenizer': 1,\n",
       "             'algorithms': 1,\n",
       "             'Hopefully': 1,\n",
       "             ',': 1,\n",
       "             'you': 1,\n",
       "             'will': 1,\n",
       "             'be': 1,\n",
       "             'able': 1,\n",
       "             'to': 1,\n",
       "             'understand': 1,\n",
       "             'how': 1,\n",
       "             'they': 1,\n",
       "             'are': 1,\n",
       "             'trained': 1,\n",
       "             'and': 1,\n",
       "             'generate': 1,\n",
       "             'tokens': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    word_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, freq in word_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4bf7a7",
   "metadata": {},
   "source": [
    "As we saw before, the alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by `##`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133e3efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##a',\n",
       " '##b',\n",
       " '##c',\n",
       " '##d',\n",
       " '##e',\n",
       " '##f',\n",
       " '##g',\n",
       " '##h',\n",
       " '##i',\n",
       " '##k',\n",
       " '##l',\n",
       " '##m',\n",
       " '##n',\n",
       " '##o',\n",
       " '##p',\n",
       " '##r',\n",
       " '##s',\n",
       " '##t',\n",
       " '##u',\n",
       " '##v',\n",
       " '##w',\n",
       " '##y',\n",
       " '##z',\n",
       " ',',\n",
       " '.',\n",
       " 'C',\n",
       " 'F',\n",
       " 'H',\n",
       " 'T',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'w',\n",
       " 'y']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb4818",
   "metadata": {},
   "source": [
    "We also add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it‚Äôs the list `[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461f7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da21858",
   "metadata": {},
   "source": [
    "Next we need to split each word, with all the letters that are not the first prefixed by `##`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4799df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['T', '##h', '##i', '##s'],\n",
       " 'is': ['i', '##s'],\n",
       " 'the': ['t', '##h', '##e'],\n",
       " 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'],\n",
       " 'Face': ['F', '##a', '##c', '##e'],\n",
       " 'Course': ['C', '##o', '##u', '##r', '##s', '##e'],\n",
       " '.': ['.'],\n",
       " 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'],\n",
       " 'about': ['a', '##b', '##o', '##u', '##t'],\n",
       " 'tokenization': ['t',\n",
       "  '##o',\n",
       "  '##k',\n",
       "  '##e',\n",
       "  '##n',\n",
       "  '##i',\n",
       "  '##z',\n",
       "  '##a',\n",
       "  '##t',\n",
       "  '##i',\n",
       "  '##o',\n",
       "  '##n'],\n",
       " 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'],\n",
       " 'shows': ['s', '##h', '##o', '##w', '##s'],\n",
       " 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'],\n",
       " 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'],\n",
       " 'algorithms': ['a',\n",
       "  '##l',\n",
       "  '##g',\n",
       "  '##o',\n",
       "  '##r',\n",
       "  '##i',\n",
       "  '##t',\n",
       "  '##h',\n",
       "  '##m',\n",
       "  '##s'],\n",
       " 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'],\n",
       " ',': [','],\n",
       " 'you': ['y', '##o', '##u'],\n",
       " 'will': ['w', '##i', '##l', '##l'],\n",
       " 'be': ['b', '##e'],\n",
       " 'able': ['a', '##b', '##l', '##e'],\n",
       " 'to': ['t', '##o'],\n",
       " 'understand': ['u',\n",
       "  '##n',\n",
       "  '##d',\n",
       "  '##e',\n",
       "  '##r',\n",
       "  '##s',\n",
       "  '##t',\n",
       "  '##a',\n",
       "  '##n',\n",
       "  '##d'],\n",
       " 'how': ['h', '##o', '##w'],\n",
       " 'they': ['t', '##h', '##e', '##y'],\n",
       " 'are': ['a', '##r', '##e'],\n",
       " 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'],\n",
       " 'and': ['a', '##n', '##d'],\n",
       " 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'],\n",
       " 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {word: [c if i == 0 else f'##{c}' for i, c in enumerate(word)] \n",
    "          for word in word_freqs.keys()}\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67896164",
   "metadata": {},
   "source": [
    "Now that we are ready for training, let‚Äôs write a function that computes the score of each pair. We‚Äôll need to use this at each step of the training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e440bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[(split[i], split[i + 1])] += freq\n",
    "            i += 1\n",
    "        letter_freqs[split[i]] += freq\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]]) \n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc338059",
   "metadata": {},
   "source": [
    "Let‚Äôs have a look at a part of this dictionary after the initial splits:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc764dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', '##h'): 0.125\n",
      "('##h', '##i'): 0.03409090909090909\n",
      "('##i', '##s'): 0.02727272727272727\n",
      "('i', '##s'): 0.1\n",
      "('t', '##h'): 0.03571428571428571\n",
      "('##h', '##e'): 0.011904761904761904\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152487d",
   "metadata": {},
   "source": [
    "Now, finding the pair with the best score only takes a quick loop:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20b1e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '##b') 0.2\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7f7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append('ab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ae1de",
   "metadata": {},
   "source": [
    "To continue, we need to apply that merge in our `splits` dictionary. Let‚Äôs write another function for this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff8af449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                res = a + b[2:] if b.startswith('##') else a + b\n",
    "                split = split[:i] + [res] + split[i + 2:]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1292dec",
   "metadata": {},
   "source": [
    "And we can have a look at the result of the first merge:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09196209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', '##o', '##u', '##t']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"a\", \"##b\", splits)\n",
    "splits[\"about\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0669a8",
   "metadata": {},
   "source": [
    "Now we have everything we need to loop until we have learned all the merges we want. Let‚Äôs aim for a vocab size of 70:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea8b7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 70\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair = \"\"\n",
    "    max_score = None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merge = best_pair[0] + best_pair[1][2:] if \\\n",
    "        best_pair[1].startswith('##') else best_pair[0] + best_pair[1]\n",
    "    vocab.append(merge)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b023bfa",
   "metadata": {},
   "source": [
    "We can then look at the generated vocabulary:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92b63e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[UNK]',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " '[MASK]',\n",
       " '##a',\n",
       " '##b',\n",
       " '##c',\n",
       " '##d',\n",
       " '##e',\n",
       " '##f',\n",
       " '##g',\n",
       " '##h',\n",
       " '##i',\n",
       " '##k',\n",
       " '##l',\n",
       " '##m',\n",
       " '##n',\n",
       " '##o',\n",
       " '##p',\n",
       " '##r',\n",
       " '##s',\n",
       " '##t',\n",
       " '##u',\n",
       " '##v',\n",
       " '##w',\n",
       " '##y',\n",
       " '##z',\n",
       " ',',\n",
       " '.',\n",
       " 'C',\n",
       " 'F',\n",
       " 'H',\n",
       " 'T',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'w',\n",
       " 'y',\n",
       " 'ab',\n",
       " '##fu',\n",
       " 'Fa',\n",
       " 'Fac',\n",
       " '##ct',\n",
       " '##ful',\n",
       " '##full',\n",
       " '##fully',\n",
       " 'Th',\n",
       " 'ch',\n",
       " '##hm',\n",
       " 'cha',\n",
       " 'chap',\n",
       " 'chapt',\n",
       " '##thm',\n",
       " 'Hu',\n",
       " 'Hug',\n",
       " 'Hugg',\n",
       " 'sh',\n",
       " 'th',\n",
       " 'is',\n",
       " '##thms',\n",
       " '##za',\n",
       " '##zat',\n",
       " '##ut']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1188b381",
   "metadata": {},
   "source": [
    "üí° Using `train_new_from_iterator()` on the same corpus won‚Äôt result in the exact same vocabulary. This is because the ü§ó Tokenizers library does not implement WordPiece for the training (since we are not completely sure of its internals), but uses BPE instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a05002",
   "metadata": {},
   "source": [
    "To tokenize a new text, we pre-tokenize it, split it, then apply the tokenization algorithm on each word. That is, we look for the biggest subword starting at the beginning of the first word and split it, then we repeat the process on the second part, and so on for the rest of that word and the following words in the text:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56810f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return ['[UNK]']\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a3201",
   "metadata": {},
   "source": [
    "Let‚Äôs test it on one word that‚Äôs in the vocabulary, and another that isn‚Äôt:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f759625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hugg', '##i', '##n', '##g']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hugging\"))\n",
    "print(encode_word(\"HOgging\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35831e44",
   "metadata": {},
   "source": [
    "Now, let‚Äôs write a function that tokenizes a text:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22199ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenize_word = [word for word, freq in pre_tokenize_result]\n",
    "    encode_words = [encode_word(word) for word in pre_tokenize_word]\n",
    "    return sum(encode_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd506e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Th',\n",
       " '##i',\n",
       " '##s',\n",
       " 'is',\n",
       " 'th',\n",
       " '##e',\n",
       " 'Hugg',\n",
       " '##i',\n",
       " '##n',\n",
       " '##g',\n",
       " 'Fac',\n",
       " '##e',\n",
       " 'c',\n",
       " '##o',\n",
       " '##u',\n",
       " '##r',\n",
       " '##s',\n",
       " '##e',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is the Hugging Face course!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
