{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca39b8b",
   "metadata": {},
   "source": [
    "# Fine-tuning a model with the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc71807",
   "metadata": {},
   "source": [
    "ü§ó Transformers provides a `Trainer` class to help you fine-tune any of the pretrained models it provides on your dataset with modern best practices. Once you‚Äôve done all the data preprocessing work in the last section, you have just a few steps left to define the `Trainer`. The hardest part is likely to be preparing the environment to run `Trainer.train()`, as it will run very slowly on a CPU. If you don‚Äôt have a GPU set up, you can get access to free GPUs or TPUs on Google Colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dec7f4",
   "metadata": {},
   "source": [
    "The code examples below assume you have already executed the examples in the previous section. Here is a short summary recapping what you need:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184bcb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.7.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.2.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (5.29.6)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers[sentencepiece]) (8.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572ff8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34668deeed7a403d866e95fb1cda43c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffa0495811f41e6ab8b9e7f47be60bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mrpc/train-00000-of-00001.parquet:   0%|          | 0.00/649k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afe580cdba44acb84b08885dc8f169b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mrpc/validation-00000-of-00001.parquet:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebf75cb45f14e94bb80d2ba62f3e87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mrpc/test-00000-of-00001.parquet:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452bd089bb1c4d2eaee057cce8af1882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d820d8538c48de9dfeae07e6f4e56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52eee997e3254aa49ea48387a85e2835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d96cad0b1746a18d7a804ea26991e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13935ef1f1f14300bda1a629ccc2c948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a3dbb1cee74c2b83af6ca23464a57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d818b526d774dc3a7f59c89bb6f0686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fbc62af6774d37a0f5fd9365655eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99341774b9934bfe9b989077149fa35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1a51289ebd4e12b8d55d3ba67323b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56420aec",
   "metadata": {},
   "source": [
    "## 1. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85098f",
   "metadata": {},
   "source": [
    "The first step before we can define our `Trainer` is to define a `TrainingArguments` class that will contain all the hyperparameters the `Trainer` will use for training and evaluation. The only argument you have to provide is a directory where the trained model will be saved, as well as the checkpoints along the way. For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87faee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "trainng_args = TrainingArguments(\n",
    "    output_dir=\"trainer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a49217",
   "metadata": {},
   "source": [
    "If you want to automatically upload your model to the Hub during training, pass along `push_to_hub=True` in the `TrainingArguments`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c52c94",
   "metadata": {},
   "source": [
    "The second step is to define our model. As in the previous chapter, we will use the `AutoModelForSequenceClassification` class, with two labels:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d35de2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d4317096cc43d0b08e8b02aec721c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e0aed6d8db40539e6e43c162dd4baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification LOAD REPORT from: bert-base-cased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def78b4",
   "metadata": {},
   "source": [
    "You will get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7d299",
   "metadata": {},
   "source": [
    "Once we have our model, we can define a `Trainer` by passing it all the objects constructed up to now ‚Äî the model, the `training_args`, the training and validation datasets, our `data_collator`, and our `processing_class`. The `processing_class` parameter is a newer addition that tells the `Trainer` which tokenizer to use for processing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db8a47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    trainng_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50e2da",
   "metadata": {},
   "source": [
    "When you pass a tokenizer as the `processing_class`, the default `data_collator` used by the `Trainer` will be a `DataCollatorWithPadding`. You can skip the `data_collator=data_collator` line in this case, but we included it here to show you this important part of the processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a944ac",
   "metadata": {},
   "source": [
    "To fine-tune the model on our dataset, we just have to call the `train()` method of our `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a1632",
   "metadata": {},
   "source": [
    "This will start the fine-tuning (which should take a couple of minutes on a GPU) and report the training loss every 500 steps. It won‚Äôt, however, tell you how well (or badly) your model is performing. This is because:\n",
    "\n",
    "1. We didn‚Äôt tell the `Trainer` to evaluate during training by setting `eval_strategy` in `TrainingArguments` to either \"steps\" (evaluate every `eval_steps`) or \"epoch\" (evaluate at the end of each epoch).\n",
    "\n",
    "2. We didn‚Äôt provide the `Trainer` with a `compute_metrics()` function to calculate a metric during said evaluation (otherwise the evaluation would just have printed the loss, which is not a very intuitive number)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf8c2a",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da678ab4",
   "metadata": {},
   "source": [
    "Let‚Äôs see how we can build a useful `compute_metrics()` function and use it the next time we train. The function must take an `EvalPrediction` object (which is a named tuple with a `predictions` field and a `label_ids` field) and will return a dictionary mapping strings to floats (the strings being the names of the metrics returned, and the floats their values). To get some predictions from our model, we can use the `Trainer.predict()` command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "701daecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_dataset[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4954ff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2)\n",
      "(408,)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.predictions.shape)\n",
    "print(predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48eaec34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0200864   1.2071925 ]\n",
      " [ 0.60737586 -0.785268  ]\n",
      " [-0.8965366   1.2125367 ]\n",
      " [-1.0168092   1.2354373 ]\n",
      " [-0.97496283  1.2462667 ]]\n",
      "[1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions.predictions[:5])\n",
    "print(predictions.label_ids[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0211eb",
   "metadata": {},
   "source": [
    "The output of the `predict()` method is another named tuple with three fields: `predictions`, `label_ids`, and `metrics`. The `metrics` field will just contain the loss on the dataset passed, as well as some time metrics (how long it took to predict, in total and on average). Once we complete our `compute_metrics()` function and pass it to the `Trainer`, that field will also contain the metrics returned by `compute_metrics()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120cde7",
   "metadata": {},
   "source": [
    " To transform logits into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9a3b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1]\n",
      "[1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "print(preds[:5])\n",
    "print(predictions.label_ids[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9b384",
   "metadata": {},
   "source": [
    "We can now compare those preds to the labels. To build our `compute_metric()` function, we will rely on the metrics from the ü§ó Evaluate library. We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the `evaluate.load()` function. The object returned has a `compute()` method we can use to do the metric calculation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59445ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7892156862745098, 'f1': 0.8594771241830066}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3421130c",
   "metadata": {},
   "source": [
    "Wrapping everything together, we get our `compute_metrics()` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c29be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(eval_preds: EvalPrediction) -> dict[str, float]:\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dbeb97",
   "metadata": {},
   "source": [
    "And to see it used in action to report metrics at the end of each epoch, here is how we define a new Trainer with this `compute_metrics()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed1a8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trainer\",\n",
    "    eval_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85763ad5",
   "metadata": {},
   "source": [
    "Note that we create a new `TrainingArguments` with its `eval_strategy` set to \"epoch\" and a new model ‚Äî otherwise, we would just be continuing the training of the model we have already trained. To launch a new training run, we execute:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ced1bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 03:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.460590</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.849023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.454532</td>\n",
       "      <td>0.669434</td>\n",
       "      <td>0.816176</td>\n",
       "      <td>0.875208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.321693</td>\n",
       "      <td>0.781904</td>\n",
       "      <td>0.835784</td>\n",
       "      <td>0.886633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35cb48a1f0a49e0963844a05ada69b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63258e45664478eb06a3f35074e3377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03bcfd4dcae4d66b79457ef8f0c621a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.33757612440321183, metrics={'train_runtime': 219.9944, 'train_samples_per_second': 50.019, 'train_steps_per_second': 6.259, 'total_flos': 419446300011600.0, 'train_loss': 0.33757612440321183, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa20ec",
   "metadata": {},
   "source": [
    "This time, it will report the validation loss and metrics at the end of each epoch on top of the training loss. Again, the exact accuracy/F1 score you reach might be a bit different from what we found, because of the random head initialization of the model, but it should be in the same ballpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caee76a",
   "metadata": {},
   "source": [
    "## 3. Advanced Training Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927ba12",
   "metadata": {},
   "source": [
    "The `Trainer` comes with many built-in features that make modern deep learning best practices accessible:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0d61f",
   "metadata": {},
   "source": [
    "**Mixed Precision Training**: Use `fp16=True` in your training arguments for faster training and reduced memory usage:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c40808",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"trainer\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6e4c6",
   "metadata": {},
   "source": [
    "**Gradient Accumulation**: For effective larger batch sizes when GPU memory is limited:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"test-trainer\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9adbd28",
   "metadata": {},
   "source": [
    "**Learning Rate Scheduling**: The Trainer uses linear decay by default, but you can customize this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba114d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"test-trainer\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",  # Try different schedulers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e39f54",
   "metadata": {},
   "source": [
    "## 4. Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc3f0f",
   "metadata": {},
   "source": [
    "1. The `Trainer` API provides a high-level interface that handles most training complexity\n",
    "\n",
    "2. Use `processing_class` to specify your tokenizer for proper data handling\n",
    "\n",
    "3. `TrainingArguments` controls all aspects of training: learning rate, batch size, evaluation strategy, and optimizations\n",
    "\n",
    "4. `compute_metrics` enables custom evaluation metrics beyond just training loss\n",
    "\n",
    "5. Modern features like mixed precision (`fp16=True`) and gradient accumulation can significantly improve training efficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
